# Structured Outputs by Example

This file contains all examples from the Structured Outputs by Example site.
It's organized by sections, with each example's Python code and terminal commands included.

## Table of Contents

* Getting Started
  * Getting Started with Structured Outputs
  * Installing Instructor
  * Your First Extraction
  * Understanding Response Models
  * Client Setup
* LLM Providers
  * OpenAI Integration
  * Anthropic Integration
  * Gemini Integration
  * Cohere Integration
  * Mistral Integration
  * Other Provider Integrations
* Basic Extraction Patterns
  * Simple Object Extraction
  * List Extraction
  * Simple Nested Structure
  * Field Validation
  * Optional Fields
  * Prompt Templates
* Multimodal Inputs
  * Vision
  * Image Extraction
  * Table Extraction
  * Audio Extraction
  * PDF Extraction
* Classification and Analysis
  * Simple Classification
  * Multi-label Classification
* Streaming
  * Streaming Basics
  * Streaming Lists
* Advanced Structures
  * Recursive Structures
  * Knowledge Graphs
  * Dependency Trees
  * Task Planning
  * Document Structure
* Validation
  * Validation Basics
  * Custom Validators
  * Retry Mechanisms
  * Fallback Strategies
  * Field-level Validation
* Performance and Optimization
  * Caching Responses
  * Parallel Extraction
  * Batch Processing
  * Hooks and Callbacks
  * Type Adapters
* Miscellaneous Examples
  * Working with Enums
  * Resources
  * Untitled Example

## Getting Started

Introduction to structured outputs with Instructor and setting up your environment

### Getting Started with Structured Outputs

Learn the basics of structured LLM outputs with Instructor. This guide demonstrates how to extract consistent, validated data from language models.
Large language models are powerful, but extracting structured data can be challenging.
Structured outputs solve this by having LLMs return data in consistent, machine-readable formats.

```python
from openai import OpenAI

client = OpenAI()
response = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
        {
            "role": "user",
            "content": "Extract customer: John Doe, age 35, email: john@example.com",
        }
    ],
)

print(response.choices[0].message.content)
import instructor
from openai import OpenAI
from pydantic import BaseModel, Field, EmailStr
class Customer(BaseModel):
    name: str = Field(description="Customer's full name")
    age: int = Field(description="Customer's age in years", ge=0, le=120)
    email: EmailStr = Field(description="Customer's email address")
client = instructor.from_openai(OpenAI())
customer = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
        {
            "role": "user",
            "content": "Extract customer: John Doe, age 35, email: john@example.com",
        }
    ],
    response_model=Customer,  # This is the key part
)

print(customer)  # Customer(name='John Doe', age=35, email='john@example.com')
print(f"Name: {customer.name}, Age: {customer.age}, Email: {customer.email}")
from typing import List, Optional
from pydantic import BaseModel, Field
import instructor
from openai import OpenAI
client = instructor.from_openai(OpenAI())
class Address(BaseModel):
    street: str
    city: str
    state: str
    zip_code: str


class Contact(BaseModel):
    email: Optional[str] = None
    phone: Optional[str] = None


class Person(BaseModel):
    name: str
    age: int
    occupation: str
    address: Address
    contact: Contact
    skills: List[str] = Field(description="List of professional skills")
person = client.chat.completions.create(
    model="gpt-4",
    messages=[
        {
            "role": "user",
            "content": """
        Extract detailed information for this person:
        John Smith is a 42-year-old software engineer living at 123 Main St, San Francisco, CA 94105.
        His email is john.smith@example.com and phone is 555-123-4567.
        John is skilled in Python, JavaScript, and cloud architecture.
        """,
        }
    ],
    response_model=Person,
)
print(f"Name: {person.name}")
print(f"Location: {person.address.city}, {person.address.state}")
print(f"Skills: {', '.join(person.skills)}")
```

```shell
$ pip install instructor pydantic
$ python getting-started.py
```

For more information, see the original documentation:
- https://github.com/jxnl/instructor

### Installing Instructor

```shell
$ pip install instructor
$ pip install instructor
$ export OPENAI_API_KEY=your_openai_key
$ pip install "instructor[anthropic]"
$ export ANTHROPIC_API_KEY=your_anthropic_key
$ pip install "instructor[google-generativeai]"
export GOOGLE_API_KEY=your_google_key
$ pip install "instructor[cohere]"
$ export COHERE_API_KEY=your_cohere_key
$ pip install "instructor[mistralai]"
$ export MISTRAL_API_KEY=your_mistral_key
$ pip install "instructor[litellm]"
```

For more information, see the original documentation:
- https://github.com/jxnl/instructor

### Your First Extraction

```python
from pydantic import BaseModel

class Person(BaseModel):
    name: str
    age: int

import instructor
from openai import OpenAI
client = instructor.from_openai(OpenAI())
person = client.chat.completions.create(
    model="gpt-3.5-turbo",
    response_model=Person,
    messages=[
        {"role": "user", "content": "John Doe is 30 years old"}
    ]
)

print(f"Name: {person.name}, Age: {person.age}")
```

```shell
$ pip install instructor pydantic
$ python first-extraction.py
```

For more information, see the original documentation:
- https://github.com/jxnl/instructor

### Understanding Response Models

```python
from pydantic import BaseModel

class User(BaseModel):
    name: str
    age: int

from pydantic import BaseModel
from typing import List, Optional

class Address(BaseModel):
    street: str
    city: str
    state: Optional[str] = None
    country: str

class User(BaseModel):
    name: str
    age: int
    addresses: List[Address]
from pydantic import BaseModel, Field

class WeatherForecast(BaseModel):
    """Weather forecast for a specific location"""

    temperature: float = Field(
        description="Current temperature in Celsius"
    )
    condition: str = Field(
        description="Weather condition (sunny, cloudy, rainy, etc.)"
    )
    humidity: int = Field(
        description="Humidity percentage from 0-100"
    )
from pydantic import BaseModel, Field

class Product(BaseModel):
    name: str = Field(min_length=3)
    price: float = Field(gt=0)  # greater than 0
    quantity: int = Field(ge=0)  # greater than or equal to 0
    description: str = Field(max_length=500)

import instructor
from openai import OpenAI

client = instructor.from_openai(OpenAI())

forecast = client.chat.completions.create(
    model="gpt-3.5-turbo",
    response_model=WeatherForecast,
    messages=[
        {"role": "user", "content": "What's the weather in New York today?"}
    ]
)

print(forecast.model_dump_json(indent=2))
```

```shell
$ pip install instructor pydantic
$ python response-models.py
```

For more information, see the original documentation:
- https://github.com/jxnl/instructor

### Client Setup

```python
import instructor
from openai import OpenAI
client = instructor.from_openai(OpenAI())
client = instructor.from_openai(
    OpenAI(),
    mode=instructor.Mode.JSON  # Use JSON mode instead
)

import instructor
from anthropic import Anthropic
client = instructor.from_anthropic(Anthropic())
client = instructor.from_anthropic(
    Anthropic(),
    mode=instructor.Mode.JSON
)

import instructor
import google.generativeai as genai
genai.configure(api_key="YOUR_API_KEY")
model = genai.GenerativeModel("gemini-1.5-flash")
client = instructor.from_gemini(
    model,
    mode=instructor.Mode.GEMINI_TOOLS  # or GEMINI_JSON
)

import instructor
import cohere
cohere_client = cohere.Client("YOUR_API_KEY")
client = instructor.from_cohere(cohere_client)

import instructor
from mistralai.client import MistralClient

mistral_client = MistralClient(api_key="YOUR_API_KEY")
client = instructor.from_mistral(mistral_client)
from instructor import Mode
Mode.TOOLS         # OpenAI function calling format (default for OpenAI)
Mode.JSON          # Plain JSON generation
Mode.MD_JSON       # Markdown JSON (used by some providers)
Mode.ANTHROPIC_TOOLS # Claude tools mode (default for Anthropic)
Mode.GEMINI_TOOLS  # Gemini tools format
Mode.GEMINI_JSON   # Gemini JSON format
```

```shell
$ pip install instructor pydantic
$ python client-setup.py
```

For more information, see the original documentation:
- https://github.com/jxnl/instructor

## LLM Providers

Using Instructor with different LLM providers

### OpenAI Integration

```python
import instructor
from openai import OpenAI
from pydantic import BaseModel

class User(BaseModel):
    name: str
    age: int

client = instructor.from_openai(OpenAI())
user = client.chat.completions.create(
    model="gpt-3.5-turbo",
    response_model=User,
    messages=[
        {"role": "user", "content": "Extract: John is a 25-year-old engineer."}
    ]
)
user = client.chat.completions.create(
    model="gpt-4",
    response_model=User,
    messages=[
        {"role": "user", "content": "Extract: John is a 25-year-old engineer."}
    ]
)

user = client.chat.completions.create(
    model="gpt-3.5-turbo",
    response_model=User,
    messages=[
        {"role": "system", "content": "You are an expert at data extraction."},
        {"role": "user", "content": "Extract user details from: John is 25 years old."}
    ]
)
user = client.chat.completions.create(
    model="gpt-3.5-turbo",
    temperature=0.1,  # Very deterministic (0.0-1.0)
    response_model=User,
    messages=[
        {"role": "user", "content": "Extract: John is a 25-year-old engineer."}
    ]
)
client = instructor.from_openai(
    OpenAI(),
    mode=instructor.Mode.JSON
)

user = client.chat.completions.create(
    model="gpt-3.5-turbo",
    response_model=User,
    messages=[
        {"role": "user", "content": "Extract: John is a 25-year-old engineer."}
    ]
)

import asyncio
from openai import AsyncOpenAI

async_client = instructor.from_openai(AsyncOpenAI())

async def extract_user():
    return await async_client.chat.completions.create(
        model="gpt-3.5-turbo",
        response_model=User,
        messages=[
            {"role": "user", "content": "Extract: John is a 25-year-old engineer."}
        ]
    )

user = asyncio.run(extract_user())
```

```shell
$ pip install instructor pydantic
$ python openai.py
```

For more information, see the original documentation:
- https://github.com/jxnl/instructor

### Anthropic Integration

```python
import instructor
from anthropic import Anthropic
from pydantic import BaseModel


class User(BaseModel):
    name: str
    age: int
client = instructor.from_anthropic(Anthropic())
user = client.messages.create(
    model="claude-3-haiku-20240307",
    max_tokens=1000,
    response_model=User,
    messages=[{"role": "user", "content": "Extract: John is 25 years old."}],
)
user = client.messages.create(
    model="claude-3-sonnet-20240229",
    max_tokens=1000,
    response_model=User,
    messages=[{"role": "user", "content": "Extract: John is 25 years old."}],
)
user = client.messages.create(
    model="claude-3-opus-20240229",
    max_tokens=1000,
    response_model=User,
    messages=[{"role": "user", "content": "Extract: John is 25 years old."}],
)

user = client.messages.create(
    model="claude-3-haiku-20240307",
    max_tokens=1000,
    response_model=User,
    system="You are an expert at data extraction. Always extract all details accurately.",
    messages=[{"role": "user", "content": "Extract: John is 25 years old."}],
)

user = client.messages.create(
    model="claude-3-haiku-20240307",
    max_tokens=1000,
    temperature=0.2,  # Lower temperature for more consistent results
    response_model=User,
    messages=[{"role": "user", "content": "Extract: John is 25 years old."}],
)
client = instructor.from_anthropic(Anthropic())
json_client = instructor.from_anthropic(Anthropic(), mode=instructor.Mode.JSON)
md_client = instructor.from_anthropic(Anthropic(), mode=instructor.Mode.MD_JSON)

import asyncio
from anthropic import AsyncAnthropic

async_client = instructor.from_anthropic(AsyncAnthropic())


async def extract_user():
    return await async_client.messages.create(
        model="claude-3-haiku-20240307",
        max_tokens=1000,
        response_model=User,
        messages=[{"role": "user", "content": "Extract: John is 25 years old."}],
    )


user = asyncio.run(extract_user())
```

```shell
$ pip install instructor pydantic
$ python anthropic.py
```

For more information, see the original documentation:
- https://github.com/jxnl/instructor

### Gemini Integration

```python
import instructor
from google import genai
from pydantic import BaseModel
API_KEY = "YOUR_API_KEY"
client = genai.Client(api_key=API_KEY)
client = instructor.from_genai(client, mode=instructor.Mode.GENAI_TOOLS)
client = genai.Client(vertexai=True, project="your-project-id", location="us-central1")
client = instructor.from_genai(client, mode=instructor.Mode.GENAI_TOOLS)
class User(BaseModel):
    name: str
    age: int


response = client.chat.completions.create(
    model="gemini-2.0-flash-001",
    messages=[{"role": "user", "content": "Extract: Jason is 25 years old"}],
    response_model=User,
)
print(response)  # User(name='Jason', age=25)
print(f"Name: {response.name}, Age: {response.age}")  # Name: Jason , Age: 25
class User(BaseModel):
    name: str
    age: int

    class Config:
        propertyOrdering = ["name", "age"]


response = client.chat.completions.create(
    model="gemini-2.0-flash-001",
    messages=[{"role": "user", "content": "Extract: Jason is 25 years old"}],
    response_model=User,
)
print(response)  # User(name='Jason', age=25)
print(f"Name: {response.name}, Age: {response.age}")  # Name: Jason, Age: 25
response = client.chat.completions.create(
    model="gemini-2.0-flash-001",
    # system="Make everyone 10 years younger you find!",
    messages=[
        {
            "role": "system",
            "content": "Make everyone 10 years younger you find!",
        },
        {
            "role": "user",
            "content": "Extract: Jason is 25 years old",
        },
    ],
    response_model=User,
)
print(response)  # User(name='Jason', age=15)
print(f"Name: {response.name}, Age: {response.age}")  # Name: Jason, Age: 15
```

```shell
$ # Install required packages
$ pip install "instructor[google-genai]"
```

For more information, see the original documentation:
- https://github.com/jxnl/instructor

### Cohere Integration

```python
import instructor
import cohere
from pydantic import BaseModel

class User(BaseModel):
    name: str
    age: int
co = cohere.Client("YOUR_API_KEY")  # or set CO_API_KEY env variable
client = instructor.from_cohere(co)
user = client.chat.completions.create(
    model="command-r-plus",  # or other Cohere models
    response_model=User,
    messages=[
        {"role": "user", "content": "Extract: John is 25 years old."}
    ]
)

print(f"Name: {user.name}, Age: {user.age}")
user = client.chat.completions.create(
    model="command",
    response_model=User,
    messages=[
        {"role": "user", "content": "Extract: John is 25 years old."}
    ]
)
user = client.chat.completions.create(
    model="command-r",
    response_model=User,
    messages=[
        {"role": "user", "content": "Extract: John is 25 years old."}
    ]
)
user = client.chat.completions.create(
    model="command-r-plus",
    response_model=User,
    messages=[
        {"role": "user", "content": "Extract: John is 25 years old."}
    ]
)

user = client.chat.completions.create(
    model="command-r-plus",
    temperature=0.2,  # Lower for more consistent results
    response_model=User,
    messages=[
        {"role": "user", "content": "Extract: John is 25 years old."}
    ]
)

user = client.chat.completions.create(
    model="command-r-plus",
    response_model=User,
    preamble="You are an expert at extracting structured information.",
    messages=[
        {"role": "user", "content": "Extract: John is 25 years old."}
    ]
)

user = client.chat.completions.create(
    model="command-r-plus",
    response_model=User,
    messages=[
        {"role": "user", "content": "Hi, I'd like to discuss John who is 25 years old."},
        {"role": "assistant", "content": "Hello! I'd be happy to discuss John with you."},
        {"role": "user", "content": "Can you extract his information in a structured format?"}
    ]
)
client = instructor.from_cohere(co)
client = instructor.from_cohere(
    co,
    mode=instructor.Mode.JSON
)
```

```shell
$ # Install required packages
$ pip install instructor cohere
```

For more information, see the original documentation:
- https://github.com/jxnl/instructor

### Mistral Integration

```python
import instructor
from mistralai.client import MistralClient
from pydantic import BaseModel

class User(BaseModel):
    name: str
    age: int
mistral_client = MistralClient(api_key="YOUR_API_KEY")
client = instructor.from_mistral(mistral_client)
user = client.chat.completions.create(
    model="mistral-large-latest",
    response_model=User,
    messages=[
        {"role": "user", "content": "Extract: John is 25 years old."}
    ]
)

print(f"Name: {user.name}, Age: {user.age}")
user = client.chat.completions.create(
    model="mistral-small-latest",
    response_model=User,
    messages=[
        {"role": "user", "content": "Extract: John is 25 years old."}
    ]
)
user = client.chat.completions.create(
    model="mistral-medium-latest",
    response_model=User,
    messages=[
        {"role": "user", "content": "Extract: John is 25 years old."}
    ]
)
user = client.chat.completions.create(
    model="mistral-large-latest",
    response_model=User,
    messages=[
        {"role": "user", "content": "Extract: John is 25 years old."}
    ]
)

user = client.chat.completions.create(
    model="mistral-large-latest",
    response_model=User,
    messages=[
        {"role": "system", "content": "You are an expert at data extraction."},
        {"role": "user", "content": "Extract: John is 25 years old."}
    ]
)

user = client.chat.completions.create(
    model="mistral-large-latest",
    temperature=0.2,  # Lower for more consistent results
    response_model=User,
    messages=[
        {"role": "user", "content": "Extract: John is 25 years old."}
    ]
)

user = client.chat.completions.create(
    model="mistral-large-latest",
    response_model=User,
    messages=[
        {"role": "user", "content": "Hi, I'd like to discuss John who is 25 years old."},
        {"role": "assistant", "content": "Hello! I'd be happy to discuss John with you."},
        {"role": "user", "content": "Can you extract his information in a structured format?"}
    ]
)
client = instructor.from_mistral(mistral_client)
client = instructor.from_mistral(
    mistral_client,
    mode=instructor.Mode.JSON
)
client = instructor.from_mistral(
    mistral_client,
    mode=instructor.Mode.MD_JSON
)

import asyncio
from mistralai.async_client import MistralAsyncClient

async_client = instructor.from_mistral(
    MistralAsyncClient(api_key="YOUR_API_KEY")
)

async def extract_user():
    return await async_client.chat.completions.create(
        model="mistral-large-latest",
        response_model=User,
        messages=[
            {"role": "user", "content": "Extract: John is 25 years old."}
        ]
    )

user = asyncio.run(extract_user())
```

```shell
$ # Install required packages
$ pip install instructor mistralai
```

For more information, see the original documentation:
- https://github.com/jxnl/instructor

### Other Provider Integrations

```python
import instructor
from litellm import completion
from pydantic import BaseModel

class User(BaseModel):
    name: str
    age: int
client = instructor.from_litellm(completion)
user = client.chat.completions.create(
    model="gpt-3.5-turbo",  # or any other provider/model combination
    response_model=User,
    messages=[
        {"role": "user", "content": "John is 25 years old."}
    ]
)

import instructor
from pydantic import BaseModel
from vertexai.preview.generative_models import GenerativeModel

class User(BaseModel):
    name: str
    age: int
model = GenerativeModel("gemini-1.5-flash")
client = instructor.from_vertexai(model)
user = client.generate_content(
    response_model=User,
    contents="Extract the user info: John is 25 years old."
)

import instructor
from openai import OpenAI
from pydantic import BaseModel

class User(BaseModel):
    name: str
    age: int
client = instructor.from_perplexity(
    OpenAI(base_url="https://api.perplexity.ai", api_key="YOUR_API_KEY")
)
user = client.chat.completions.create(
    model="sonar",  # or other Perplexity models
    response_model=User,
    messages=[
        {"role": "user", "content": "John is 25 years old."}
    ]
)

import instructor
from openai import OpenAI
from pydantic import BaseModel

class User(BaseModel):
    name: str
    age: int
client = instructor.from_fireworks(
    OpenAI(base_url="https://api.fireworks.ai/inference/v1", api_key="YOUR_API_KEY")
)
user = client.chat.completions.create(
    model="accounts/fireworks/models/mixtral-8x7b-instruct",
    response_model=User,
    messages=[
        {"role": "user", "content": "John is 25 years old."}
    ]
)

import instructor
from openai import OpenAI
from pydantic import BaseModel

class User(BaseModel):
    name: str
    age: int
client = instructor.from_anyscale(
    OpenAI(base_url="https://api.endpoints.anyscale.com/v1", api_key="YOUR_API_KEY")
)
user = client.chat.completions.create(
    model="meta-llama/Llama-3-8b-instruct",
    response_model=User,
    messages=[
        {"role": "user", "content": "John is 25 years old."}
    ]
)

import instructor
from openai import OpenAI
from pydantic import BaseModel

class User(BaseModel):
    name: str
    age: int
client = instructor.from_together(
    OpenAI(base_url="https://api.together.xyz/v1", api_key="YOUR_API_KEY")
)
user = client.chat.completions.create(
    model="togethercomputer/llama-3-8b-instructk",
    response_model=User,
    messages=[
        {"role": "user", "content": "John is 25 years old."}
    ]
)

import instructor
from openai import OpenAI
from pydantic import BaseModel

class User(BaseModel):
    name: str
    age: int
client = instructor.from_openrouter(
    OpenAI(base_url="https://openrouter.ai/api/v1", api_key="YOUR_API_KEY")
)
user = client.chat.completions.create(
    model="google/gemma-7b-instruct", # Or any other model on OpenRouter
    response_model=User,
    messages=[
        {"role": "user", "content": "John is 25 years old."}
    ]
)
```

```shell
$ pip install instructor pydantic
$ python other-providers.py
```

For more information, see the original documentation:
- https://github.com/jxnl/instructor

## Basic Extraction Patterns

Common patterns for extracting structured data

### Simple Object Extraction

Here we extract basic objects from text with Instructor

```python
from pydantic import BaseModel, Field
import instructor
from openai import OpenAI
class Person(BaseModel):
    """Extract person information from text."""
    name: str = Field(description="The person's full name")
    age: int = Field(description="The person's age in years")
    occupation: str = Field(description="The person's current job title or role")
client = instructor.from_openai(OpenAI())
person = client.chat.completions.create(
    model="gpt-3.5-turbo",
    response_model=Person,
    messages=[
        {"role": "user", "content": "John Doe is a 30-year-old software engineer."}
    ]
)
print(f"Name: {person.name}")
print(f"Age: {person.age}")
print(f"Occupation: {person.occupation}")
```

```shell
$ pip install instructor pydantic
$ python simple-object.py
```

For more information, see the original documentation:
- https://github.com/jxnl/instructor

### List Extraction

You are also able to extract lists of objects from text with Instructor.

```python
from pydantic import BaseModel
import instructor
from openai import OpenAI
from typing import List
class Person(BaseModel):
    name: str
    age: int
client = instructor.from_openai(OpenAI())
people = client.chat.completions.create(
    model="gpt-3.5-turbo",
    response_model=List[Person],  # Note the List wrapper
    messages=[
        {"role": "user", "content": """
        Extract all people mentioned in this text:
        - John is 30 years old
        - Mary is 25 years old
        - Bob is 45 years old
        """}
    ]
)
for person in people:
    print(f"{person.name} is {person.age} years old")
```

```shell
$ pip install instructor pydantic
$ python list-extraction.py
```

For more information, see the original documentation:
- https://github.com/jxnl/instructor

### Simple Nested Structure

This example shows how to work with nested objects in Instructor.

```python
import instructor
from openai import OpenAI
from pydantic import BaseModel
class Address(BaseModel):
    street: str
    city: str
    zip_code: str

class Person(BaseModel):
    name: str
    age: int
    address: Address  # Nested object
client = instructor.from_openai(OpenAI())
person = client.chat.completions.create(
    model="gpt-3.5-turbo",
    response_model=Person,
    messages=[
        {"role": "user", "content": """
        John Smith is 35 years old and lives at
        123 Main Street, New York, 10001.
        """}
    ]
)
print(f"Name: {person.name}")
print(f"Age: {person.age}")
print(f"Street: {person.address.street}")
print(f"City: {person.address.city}")
print(f"ZIP: {person.address.zip_code}")
```

```shell
$ pip install instructor pydantic
$ python nested-structures.py
```

For more information, see the original documentation:
- https://github.com/jxnl/instructor

### Field Validation

```python
from pydantic import BaseModel, Field
import instructor
from openai import OpenAI
class Product(BaseModel):
    name: str = Field(min_length=3, max_length=50)
    price: float = Field(gt=0)  # must be greater than 0
    quantity: int = Field(ge=0)  # must be greater than or equal to 0
    category: str
client = instructor.from_openai(OpenAI())
product = client.chat.completions.create(
    model="gpt-3.5-turbo",
    response_model=Product,
    messages=[
        {"role": "user", "content": "We sell a premium coffee mug for $12.99 and have 25 in stock in our kitchen category."}
    ]
)

print(f"Name: {product.name}")
print(f"Price: ${product.price}")
print(f"Quantity: {product.quantity}")
print(f"Category: {product.category}")

from pydantic import BaseModel, Field

class PersonStats(BaseModel):
    name: str
    age: int = Field(ge=0, lt=120)  # 0 ≤ age < 120
    height: float = Field(gt=0, le=300)  # 0 < height ≤ 300 (cm)
    weight: float = Field(gt=0, le=500)  # 0 < weight ≤ 500 (kg)
    body_temperature: float = Field(ge=35, le=42)  # normal human range in Celsius
person = client.chat.completions.create(
    model="gpt-3.5-turbo",
    response_model=PersonStats,
    messages=[
        {"role": "user", "content": """
        Patient: John Smith
        Age: 35 years old
        Height: 180 cm
        Weight: 75 kg
        Temperature: 37.2°C
        """}
    ]
)

print(f"Patient: {person.name}")
print(f"Age: {person.age}")
print(f"Height: {person.height} cm")
print(f"Weight: {person.weight} kg")
print(f"Body Temperature: {person.body_temperature}°C")

from pydantic import BaseModel, Field, field_validator
import re

class ContactInfo(BaseModel):
    name: str
    email: str = Field(pattern=r'^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+$')
    phone: str = Field(pattern=r'^\+?[1-9]\d{1,14}$')  # E.164 phone format
    website: str = Field(pattern=r'^https?://(?:www\.)?[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}(?:/[^\s]*)?$')
@field_validator('name')
    def validate_name(cls, v):
        if len(v.split()) < 2:
            raise ValueError('Name must include at least first and last name')
        return v
contact = client.chat.completions.create(
    model="gpt-4",  # More capable for handling pattern constraints
    response_model=ContactInfo,
    messages=[
        {"role": "user", "content": """
        Contact details for our new client:
        Name: John A. Smith
        Email: john.smith@example.com
        Phone: +1-555-123-4567
        Website: https://www.johnsmith.com
        """}
    ]
)

print(f"Name: {contact.name}")
print(f"Email: {contact.email}")
print(f"Phone: {contact.phone}")
print(f"Website: {contact.website}")
from pydantic import BaseModel, Field

class User(BaseModel):
    name: str
    age: int = Field(ge=18, le=100)  # Must be between 18 and 100
    email: str = Field(pattern=r'^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+$')
user = client.chat.completions.create(
    model="gpt-3.5-turbo",
    response_model=User,
    max_retries=2,  # Limit retries (default is 3)
    messages=[
        {"role": "user", "content": "Sam is 16 years old and his email is sam@example"}
    ]
)
print(f"Name: {user.name}")
print(f"Age: {user.age}")  # Should be adjusted to valid range
print(f"Email: {user.email}")  # Should include a valid domain

from pydantic import BaseModel, Field, field_validator
from datetime import date
from typing import Optional

class Reservation(BaseModel):
    guest_name: str
    check_in_date: date
    check_out_date: date
    room_type: str
    num_guests: int = Field(gt=0)
    special_requests: Optional[str] = None

    @field_validator('check_out_date')
    def validate_dates(cls, v, values):
        if 'check_in_date' in values.data and v <= values.data['check_in_date']:
            raise ValueError('check_out_date must be after check_in_date')
        return v

    @field_validator('num_guests')
    def validate_guests(cls, v, values):
        if 'room_type' in values.data:
            if values.data['room_type'].lower() == 'single' and v > 1:
                raise ValueError('Single rooms can only accommodate 1 guest')
            elif values.data['room_type'].lower() == 'double' and v > 2:
                raise ValueError('Double rooms can only accommodate 2 guests')
        return v
reservation = client.chat.completions.create(
    model="gpt-4",
    response_model=Reservation,
    messages=[
        {"role": "user", "content": """
        Hotel reservation details:
        Guest: Maria Garcia
        Check-in: 2023-11-15
        Check-out: 2023-11-20
        Room: Double
        Guests: 2
        Special requests: Early check-in if possible
        """}
    ]
)

print(f"Guest: {reservation.guest_name}")
print(f"Stay: {reservation.check_in_date} to {reservation.check_out_date}")
print(f"Room: {reservation.room_type} for {reservation.num_guests} guests")
if reservation.special_requests:
    print(f"Special requests: {reservation.special_requests}")
```

```shell
$ pip install instructor pydantic
$ python field-validation.py
```

For more information, see the original documentation:
- https://github.com/jxnl/instructor

### Optional Fields

This example demonstrates how to handle optional or missing data in structured extractions:
1. Using Optional type annotations for fields that might not be present in the source text
2. Using Instructor's Maybe type to explicitly track whether information was present

Optional fields allow your models to gracefully handle incomplete information without
causing extraction failures when certain data isn't mentioned in the text.

```python
from pydantic import BaseModel, Field
from typing import Optional
import instructor
from openai import OpenAI
class Person(BaseModel):
    name: str
    age: int
    # Optional fields with default value of None
    email: Optional[str] = None
    phone: Optional[str] = None
    occupation: Optional[str] = None
client = instructor.from_openai(OpenAI())
person = client.chat.completions.create(
    model="gpt-3.5-turbo",
    response_model=Person,
    messages=[
        {"role": "user", "content": "John Smith is 35 years old and works as a software engineer."}
    ]
)
print(f"Name: {person.name}")
print(f"Age: {person.age}")
print(f"Email: {person.email}")  # None
print(f"Phone: {person.phone}")  # None
print(f"Occupation: {person.occupation}")  # "software engineer"
```

```shell
$ pip install instructor pydantic
$ python optional-fields.py
```

For more information, see the original documentation:
- https://github.com/jxnl/instructor

### Prompt Templates

Learn to dynamically create prompts using Jinja templating and validate them with Pydantic

```python
import openai
import instructor
from pydantic import BaseModel
from typing import List
class User(BaseModel):
    """Extract user information from text."""
    name: str
    age: int
client = instructor.from_openai(openai.OpenAI())
user = client.chat.completions.create(
    model="gpt-4o",
    messages=[
        {
            "role": "user",
            "content": """Extract the information from the
        following text: `{{ data }}`"""
        }
    ],
    response_model=User,
    context={"data": "John Doe is thirty years old"}
)
print(f"Name: {user.name}")
print(f"Age: {user.age}")
class Citation(BaseModel):
    source_id: int
    text: str

class Answer(BaseModel):
    """Extract answer with supporting citations"""
    answer: str
    citations: List[Citation]
advanced_response = client.chat.completions.create(
    model="gpt-4o",
    messages=[
        {
            "role": "user",
            "content": """
            You are a {{ role }} answering the following question:
            
            <question>
            {{ question }}
            </question>
            
            Use the following sources to answer the question:
            
            <sources>
            {% for source in sources %}
            <source id="{{ source.id }}">
            {{ source.content }}
            </source>
            {% endfor %}
            </sources>
            
            Provide a concise answer with citations to the relevant sources.
            """
        }
    ],
    response_model=Answer,
    context={
        "role": "research assistant",
        "question": "What are the capitals of France and Japan?",
        "sources": [
            {"id": 1, "content": "Paris is the capital city of France."},
            {"id": 2, "content": "France is located in Western Europe."},
            {"id": 3, "content": "Tokyo is the capital city of Japan."},
            {"id": 4, "content": "Japan is an island country in East Asia."}
        ]
    }
)
for citation in advanced_response.citations:
    print(f"  Source {citation.source_id}: {citation.text}")
    # Source 1: Paris is the capital city of France.
    # Source 2: France is located in Western Europe.
    # Source 3: Tokyo is the capital city of Japan.
    # Source 4: Japan is an island country in East Asia.
```

```shell
$ pip install instructor pydantic
$ python templates.py
```

## Multimodal Inputs

Working with images, audio, and documents

### Vision

```python
import instructor
from openai import OpenAI
from pydantic import BaseModel, Field
from typing import List
client = instructor.from_openai(OpenAI())
class ImageContent(BaseModel):
    description: str = Field(description="A detailed description of the image")
    objects: List[str] = Field(description="List of main objects in the image")
    colors: List[str] = Field(description="Dominant colors in the image")
def analyze_image_from_file(file_path: str) -> ImageContent:
    image = instructor.Image.from_path(file_path)

    return client.chat.completions.create(
        model="gpt-4-vision-preview",
        response_model=ImageContent,
        messages=[
            {
                "role": "user",
                "content": [
                    "Describe this image in detail:",
                    image  # The Image object is handled automatically
                ]
            }
        ]
    )
def analyze_image_from_url(image_url: str) -> ImageContent:
    image = instructor.Image.from_url(image_url)

    return client.chat.completions.create(
        model="gpt-4-vision-preview",
        response_model=ImageContent,
        messages=[
            {
                "role": "user",
                "content": [
                    "Describe this image in detail:",
                    image
                ]
            }
        ]
    )
def analyze_with_autodetect(image_path_or_url: str) -> ImageContent:
    return client.chat.completions.create(
        model="gpt-4-vision-preview",
        response_model=ImageContent,
        messages=[
            {
                "role": "user",
                "content": [
                    "Describe this image in detail:",
                    image_path_or_url  # Will be automatically detected as an image
                ]
            }
        ],
        autodetect_images=True  # Automatically converts paths/URLs to Image objects
    )
result = analyze_with_autodetect("https://example.com/image.jpg")
print(f"Description: {result.description}")
print(f"Objects: {', '.join(result.objects)}")
print(f"Colors: {', '.join(result.colors)}")
```

```shell
$ pip install instructor pydantic
$ python vision-inputs.py
```

For more information, see the original documentation:
- https://github.com/jxnl/instructor

### Image Extraction

```python
import instructor
from openai import OpenAI
from pydantic import BaseModel, Field
from typing import List, Optional
client = instructor.from_openai(OpenAI())
class Product(BaseModel):
    name: str = Field(description="Product name")
    price: float = Field(description="Product price in USD")
    description: str = Field(description="Brief product description")
    features: List[str] = Field(description="Key product features")
    brand: Optional[str] = Field(None, description="Brand name if visible")
def extract_product_info(image_path_or_url: str) -> Product:
    return client.chat.completions.create(
        model="gpt-4-vision-preview",
        response_model=Product,
        messages=[
            {
                "role": "user",
                "content": [
                    "Extract detailed product information from this image:",
                    image_path_or_url
                ]
            }
        ],
        autodetect_images=True  # Automatically handle the image
    )
product = extract_product_info("path/to/product_image.jpg")
print(f"Product: {product.name} (${product.price:.2f})")
print(f"Description: {product.description}")
print(f"Features: {', '.join(product.features)}")
if product.brand:
    print(f"Brand: {product.brand}")
from typing import List, Optional
from pydantic import BaseModel, Field
import instructor
from openai import OpenAI
client = instructor.from_openai(OpenAI())
class Ingredient(BaseModel):
    name: str = Field(description="Ingredient name")
    quantity: str = Field(description="Amount needed, including units")
    optional: bool = Field(description="Whether this ingredient is optional")

class Step(BaseModel):
    instruction: str = Field(description="Cooking instruction")
    time_minutes: Optional[int] = Field(None, description="Time required for this step in minutes")

class Recipe(BaseModel):
    title: str = Field(description="Recipe title")
    servings: int = Field(description="Number of servings")
    prep_time_minutes: int = Field(description="Preparation time in minutes")
    cook_time_minutes: int = Field(description="Cooking time in minutes")
    ingredients: List[Ingredient] = Field(description="List of ingredients")
    steps: List[Step] = Field(description="Cooking steps in order")
    difficulty: str = Field(description="Recipe difficulty (easy, medium, hard)")
def extract_recipe(image_path: str) -> Recipe:
    # Create an Image object
    image = instructor.Image.from_path(image_path)

    return client.chat.completions.create(
        model="gpt-4-vision-preview",
        response_model=Recipe,
        messages=[
            {
                "role": "system",
                "content": "Extract complete recipe information from the provided image."
            },
            {
                "role": "user",
                "content": [
                    "Please extract the detailed recipe information from this image:",
                    image
                ]
            }
        ]
    )
```

```shell
$ pip install instructor pydantic
$ python image-to-structured-data.py
```

For more information, see the original documentation:
- https://github.com/jxnl/instructor

### Table Extraction

```python
import instructor
from openai import OpenAI
from pydantic import BaseModel, Field
import pandas as pd
from typing import Annotated, Any
from io import StringIO
from pydantic import (
    BaseModel,
    BeforeValidator,
    PlainSerializer,
    InstanceOf,
    WithJsonSchema,
)
client = instructor.from_openai(OpenAI(), mode=instructor.Mode.MD_JSON)
def to_markdown(df: pd.DataFrame) -> str:
    """Convert a dataframe to markdown format."""
    return df.to_markdown()


def md_to_df(data: Any) -> Any:
    """Convert markdown table to a pandas dataframe."""
    if isinstance(data, str):
        return (
            pd.read_csv(
                StringIO(data),
                sep="|",
                index_col=1,
            )
            .dropna(axis=1, how="all")
            .iloc[1:]
            .map(lambda x: x.strip())
        )
    return data
MarkdownDataFrame = Annotated[
    InstanceOf[pd.DataFrame],
    BeforeValidator(md_to_df),
    PlainSerializer(to_markdown),
    WithJsonSchema(
        {
            "type": "string",
            "description": "The markdown representation of the table",
        }
    ),
]
class Table(BaseModel):
    caption: str = Field(description="A descriptive caption for the table")
    dataframe: MarkdownDataFrame = Field(
        description="The table data as a markdown table"
    )
def extract_table_from_image(image_path_or_url: str) -> Table:
    """Extract a table from an image and return it as a structured object."""
    if image_path_or_url.startswith(("http://", "https://")):
        image = instructor.Image.from_url(image_path_or_url)
    else:
        image = instructor.Image.from_path(image_path_or_url)

    return client.chat.completions.create(
        model="gpt-4-vision-preview",
        response_model=Table,
        max_tokens=1800,
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": "Extract the table from this image with a descriptive caption.",
                    },
                    image,
                ],
            }
        ],
    )
def analyze_table_data(image_path: str):
    """Extract and analyze a table from an image."""
    table = extract_table_from_image(image_path)

    print(f"Table Caption: {table.caption}")
    print("\nExtracted Table:")
    print(table.dataframe)

    # Perform data analysis if it's a pandas DataFrame
    if isinstance(table.dataframe, pd.DataFrame):
        print("\nData Analysis:")
        print(f"- Rows: {len(table.dataframe)}")
        print(f"- Columns: {len(table.dataframe.columns)}")

        # Basic statistics if numeric columns exist
        numeric_cols = table.dataframe.select_dtypes(include=["number"]).columns
        if len(numeric_cols) > 0:
            print("\nNumeric Column Statistics:")
            for col in numeric_cols:
                col_data = table.dataframe[col]
                print(
                    f"- {col}: Min={col_data.min()}, Max={col_data.max()}, Mean={col_data.mean():.2f}"
                )

        return table.dataframe

    return None
def extract_multiple_tables(image_path_or_url: str) -> list[Table]:
    """Extract all tables from an image."""
    if image_path_or_url.startswith(("http://", "https://")):
        image = instructor.Image.from_url(image_path_or_url)
    else:
        image = instructor.Image.from_path(image_path_or_url)

    tables = client.chat.completions.create_iterable(
        model="gpt-4-vision-preview",
        response_model=Table,
        max_tokens=1800,
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": "Extract all tables from this image. Each table should be separate and have its own caption.",
                    },
                    image,
                ],
            }
        ],
    )

    return list(tables)
def analyze_multiple_tables(image_path: str):
    """Extract and analyze all tables from an image."""
    tables = extract_multiple_tables(image_path)

    print(f"Found {len(tables)} tables in the image.")

    for i, table in enumerate(tables):
        print(f"\n--- Table {i+1}: {table.caption} ---")
        print(table.dataframe)

        if isinstance(table.dataframe, pd.DataFrame):
            yield table.dataframe
```

```shell
$ pip install instructor pydantic
$ python table-extraction.py
```

For more information, see the original documentation:
- https://github.com/jxnl/instructor

### Audio Extraction

```python
import instructor
from openai import OpenAI
from pydantic import BaseModel, Field
from instructor.multimodal import Audio
client = instructor.from_openai(OpenAI())
class AudioTranscription(BaseModel):
    text: str = Field(description="Full transcription of the audio")
    speaker: str = Field(description="Identity of the speaker if known")
    language: str = Field(description="Language spoken in the audio")
    confidence: float = Field(description="Confidence score for the transcription", ge=0.0, le=1.0)
def transcribe_audio(audio_path: str) -> AudioTranscription:
    """Extract structured transcription from an audio file."""
    # Load the audio using Instructor's Audio class
    audio = Audio.from_path(audio_path)

    return client.chat.completions.create(
        model="gpt-4o-audio-preview",  # Audio-capable model
        response_model=AudioTranscription,
        messages=[
            {
                "role": "user",
                "content": [
                    "Transcribe this audio file and identify the speaker and language:",
                    audio  # The Audio object is handled automatically
                ]
            }
        ]
    )
from typing import List, Optional
from pydantic import BaseModel, Field
import instructor
from openai import OpenAI
from instructor.multimodal import Audio
client = instructor.from_openai(OpenAI())
class Person(BaseModel):
    name: str = Field(description="Person's full name")
    age: int = Field(description="Person's age in years")
    occupation: Optional[str] = Field(None, description="Person's job or profession if mentioned")
class MeetingPoint(BaseModel):
    topic: str = Field(description="Topic discussed")
    decision: Optional[str] = Field(None, description="Decision made on this topic")
    action_items: List[str] = Field(default_factory=list, description="Action items related to this topic")

class Meeting(BaseModel):
    title: str = Field(description="Meeting title or purpose")
    date: Optional[str] = Field(None, description="Meeting date if mentioned")
    participants: List[str] = Field(description="Names of meeting participants")
    key_points: List[MeetingPoint] = Field(description="Key discussion points and decisions")
    summary: str = Field(description="Brief summary of the meeting")
def extract_meeting_info(audio_path: str) -> Meeting:
    """Extract structured meeting information from audio recording."""
    audio = Audio.from_path(audio_path)

    return client.chat.completions.create(
        model="gpt-4o-audio-preview",
        response_model=Meeting,
        messages=[
            {
                "role": "system",
                "content": "Extract detailed meeting information from this audio recording."
            },
            {
                "role": "user",
                "content": [
                    "Extract the complete meeting details from this recording:",
                    audio
                ]
            }
        ]
    )
def extract_person_from_audio(audio_path: str) -> Person:
    """Extract structured person information from audio."""
    audio = Audio.from_path(audio_path)

    return client.chat.completions.create(
        model="gpt-4o-audio-preview",
        response_model=Person,
        messages=[
            {
                "role": "user",
                "content": [
                    "Extract the person's name, age, and occupation from this audio:",
                    audio
                ]
            }
        ]
    )
```

```shell
$ pip install instructor pydantic
$ python audio-extraction.py
```

For more information, see the original documentation:
- https://github.com/jxnl/instructor

### PDF Extraction

```python
import instructor
from openai import OpenAI
from pydantic import BaseModel, Field
from typing import List
import tempfile
from pdf2image import convert_from_path
client = instructor.from_openai(OpenAI())
class Section(BaseModel):
    title: str = Field(description="Section title")
    content: str = Field(description="Section content")

class Document(BaseModel):
    title: str = Field(description="Document title")
    author: str = Field(description="Document author")
    sections: List[Section] = Field(description="Document sections")
    summary: str = Field(description="Brief document summary")
def extract_from_pdf_page(pdf_path: str, page_number: int = 0) -> Document:
    """Extract structured information from a PDF page."""
    # Convert the PDF page to an image
    images = convert_from_path(pdf_path, first_page=page_number+1, last_page=page_number+1)

    # Save the image to a temporary file
    with tempfile.NamedTemporaryFile(suffix='.jpg', delete=False) as temp:
        temp_path = temp.name
        images[0].save(temp_path, 'JPEG')

    # Create an Image object
    image = instructor.Image.from_path(temp_path)

    # Extract information using vision capabilities
    return client.chat.completions.create(
        model="gpt-4-vision-preview",
        response_model=Document,
        messages=[
            {
                "role": "system",
                "content": "Extract structured information from this document page."
            },
            {
                "role": "user",
                "content": [
                    "Extract the complete document structure from this page:",
                    image
                ]
            }
        ]
    )
def process_pdf_document(pdf_path: str, max_pages: int = 5) -> List[Document]:
    """Process multiple pages from a PDF document."""
    results = []

    # Get the number of pages
    from pypdf import PdfReader
    reader = PdfReader(pdf_path)
    num_pages = len(reader.pages)
    actual_pages = min(num_pages, max_pages)

    # Process each page
    for i in range(actual_pages):
        page_result = extract_from_pdf_page(pdf_path, i)
        results.append(page_result)
        print(f"Processed page {i+1}/{actual_pages}")

    return results
from typing import List, Optional
from pydantic import BaseModel, Field
import instructor
from openai import OpenAI
from pdf2image import convert_from_path
import tempfile
client = instructor.from_openai(OpenAI())
class LineItem(BaseModel):
    description: str = Field(description="Description of the item or service")
    quantity: float = Field(description="Quantity of the item")
    unit_price: float = Field(description="Price per unit")
    amount: float = Field(description="Total amount for this line")

class Invoice(BaseModel):
    invoice_number: str = Field(description="Invoice identifier")
    date: str = Field(description="Invoice date")
    vendor: str = Field(description="Name of the vendor/seller")
    customer: str = Field(description="Name of the customer/buyer")
    items: List[LineItem] = Field(description="Line items in the invoice")
    subtotal: float = Field(description="Sum of all items before tax")
    tax: Optional[float] = Field(None, description="Tax amount")
    total: float = Field(description="Total invoice amount")
def extract_invoice(pdf_path: str) -> Invoice:
    """Extract structured invoice data from a PDF."""
    # Convert first page to image
    images = convert_from_path(pdf_path, first_page=1, last_page=1)

    # Save to temp file
    with tempfile.NamedTemporaryFile(suffix='.jpg', delete=False) as temp:
        temp_path = temp.name
        images[0].save(temp_path, 'JPEG')

    # Create image object
    image = instructor.Image.from_path(temp_path)

    # Extract invoice data
    return client.chat.completions.create(
        model="gpt-4-vision-preview",
        response_model=Invoice,
        messages=[
            {
                "role": "system",
                "content": "You are an invoice processing assistant that extracts structured data from invoice images."
            },
            {
                "role": "user",
                "content": [
                    "Extract complete invoice details from this document:",
                    image
                ]
            }
        ]
    )
```

```shell
$ pip install instructor pydantic
$ python pdf-extraction.py
```

For more information, see the original documentation:
- https://github.com/jxnl/instructor

## Classification and Analysis

Using structured outputs for classification tasks

### Simple Classification

Perform single-label classification with Instructor and structured outputs.

```python
from pydantic import BaseModel, Field
from typing import Literal
import instructor
from openai import OpenAI
class Classification(BaseModel):
    """A single-label classification for text as SPAM or NOT_SPAM"""

    label: Literal["SPAM", "NOT_SPAM"] = Field(
        description="The classification label, either SPAM or NOT_SPAM"
    )
client = instructor.from_openai(OpenAI())
def classify_text(text: str) -> Classification:
    return client.chat.completions.create(
        model="gpt-3.5-turbo",
        response_model=Classification,
        messages=[
            {
                "role": "system",
                "content": """
                You are an email spam classifier. Classify the provided text as either SPAM or NOT_SPAM.

                Examples of SPAM:
                - "Claim your free prize now!"
                - "Make $1000 a day working from home"
                - "Limited time offer - 90% discount"

                Examples of NOT_SPAM:
                - "Can we schedule a meeting tomorrow?"
                - "Here's the report you requested"
                - "Please review the attached document"
                """
            },
            {"role": "user", "content": f"Classify this text: {text}"}
        ]
    )
spam_text = "URGENT: Your account has been compromised. Click here to verify details!"
legit_text = "Please review the meeting notes and provide your feedback by Friday."
spam_result = classify_text("URGENT: Your account has been compromised. Click here to verify details!")
legit_result = classify_text("Please review the meeting notes and provide your feedback by Friday.")
```

```shell
$ pip install instructor pydantic
$ python simple-classification.py
```

For more information, see the original documentation:
- https://github.com/jxnl/instructor

### Multi-label Classification

This example demonstrates various approaches to multi-label classification using Instructor:
1. Basic multi-label classification with string labels
2. Using Enums for predefined categories
3. Classification with confidence scores
4. Hierarchical classification with main and subcategories

Each approach shows how to structure your Pydantic models for different classification needs.

```python
from pydantic import BaseModel, Field
from typing import List, Optional
from enum import Enum
import instructor
from openai import OpenAI
client = instructor.from_openai(OpenAI())
class MultiLabelClassification(BaseModel):
    """Multi-label classification of text content"""
    labels: List[str] = Field(
        description="List of applicable category labels for the text"
    )
def classify_text(text: str) -> MultiLabelClassification:
    return client.chat.completions.create(
        model="gpt-3.5-turbo",
        response_model=MultiLabelClassification,
        messages=[
            {
                "role": "system",
                "content": """
                Classify the text into one or more of these categories:
                - Technology
                - Finance
                - Health
                - Sports
                - Entertainment
                - Politics
                - Science
                - Education

                Return all categories that apply to the text.
                """
            },
            {"role": "user", "content": f"Text for classification: {text}"}
        ]
    )
class Category(str, Enum):
    BUSINESS = "business"
    TECHNOLOGY = "technology"
    POLITICS = "politics"
    HEALTH = "health"
    ENTERTAINMENT = "entertainment"
    SPORTS = "sports"
    SCIENCE = "science"
    EDUCATION = "education"
class EnumMultiLabelClassification(BaseModel):
    """Multi-label classification using predefined categories"""
    categories: List[Category] = Field(
        description="List of applicable categories from the predefined set"
    )
def classify_with_enums(text: str) -> EnumMultiLabelClassification:
    return client.chat.completions.create(
        model="gpt-3.5-turbo",
        response_model=EnumMultiLabelClassification,
        messages=[
            {"role": "system", "content": "Classify the text into one or more predefined categories."},
            {"role": "user", "content": f"Text for classification: {text}"}
        ]
    )
class LabelWithConfidence(BaseModel):
    label: str
    confidence: float = Field(gt=0, le=1)  # Between 0 and 1

class ConfidenceClassification(BaseModel):
    labels: List[LabelWithConfidence] = Field(
        description="List of applicable labels with confidence scores"
    )

def classify_with_confidence(text: str) -> ConfidenceClassification:
    return client.chat.completions.create(
        model="gpt-3.5-turbo",
        response_model=ConfidenceClassification,
        messages=[
            {
                "role": "system",
                "content": """
                Classify the text into these categories and provide confidence scores (0-1):
                - Technology
                - Finance
                - Health
                - Sports
                - Entertainment
                Only include categories that apply with a confidence score over 0.4.
                """
            },
            {"role": "user", "content": f"Text for classification: {text}"}
        ]
    )
class SubCategory(BaseModel):
    name: str
    confidence: float = Field(gt=0, le=1)

class MainCategory(BaseModel):
    name: str
    confidence: float = Field(gt=0, le=1)
    subcategories: List[SubCategory] = []

class HierarchicalClassification(BaseModel):
    categories: List[MainCategory] = Field(
        description="Hierarchical categories with confidence scores"
    )
def classify_hierarchical(text: str) -> HierarchicalClassification:
    return client.chat.completions.create(
        model="gpt-4",  # More complex tasks work better with GPT-4
        response_model=HierarchicalClassification,
        messages=[
            {
                "role": "system",
                "content": """
                Classify the text into main categories and subcategories:

                Main categories:
                - Technology (subcategories: Hardware, Software, AI, Internet)
                - Science (subcategories: Physics, Biology, Chemistry, Astronomy)
                - Health (subcategories: Fitness, Nutrition, Medical, Mental Health)

                Return only relevant categories with confidence scores.
                """
            },
            {"role": "user", "content": f"Text for classification: {text}"}
        ]
    )
result = classify_text("Researchers have developed a new AI algorithm that can detect early signs of Alzheimer's disease from brain scans with 94% accuracy. The deep learning software could help doctors diagnose patients years earlier than current methods.")
```

```shell
$ pip install instructor pydantic
$ python multi-label-classification.py
```

For more information, see the original documentation:
- https://github.com/jxnl/instructor

## Streaming

Working with streaming responses

### Streaming Basics

```python
import instructor
from openai import OpenAI
from pydantic import BaseModel

class User(BaseModel):
    name: str
    age: int
    bio: str
client = instructor.from_openai(OpenAI())
def stream_user_info():
    stream = client.chat.completions.create(
        model="gpt-3.5-turbo",
        response_model=User,
        stream=True,  # Enable streaming
        messages=[
            {"role": "user", "content": "Generate a profile for a fictional user named Alice who is 28 years old."}
        ]
    )

    # Each chunk contains the partial model constructed so far
    for chunk in stream:
        print(f"Received chunk: {chunk}")
        
    # Return the final complete object
    return chunk

user = stream_user_info()
print(f"\nFinal result: {user}")

from instructor import Partial
def stream_user_with_partial():
    user_stream = client.chat.completions.create_partial(
        model="gpt-3.5-turbo",
        response_model=User,
        messages=[
            {"role": "user", "content": "Generate a profile for a fictional user named Bob who is 35 years old and works as a software developer."}
        ]
    )

    # Show progress as each field gets filled in
    print("Streaming user data:")

    for partial_user in user_stream:
        # Fields appear as they're generated by the model
        print(f"Current state: name={partial_user.name}, age={partial_user.age}, bio={partial_user.bio!r}")
from typing import Dict, Any

class ProgressTracker:
    def __init__(self):
        self.progress = {}
def update(self, partial_user: Partial[User]):
        # Calculate what percentage of fields are now populated
        total_fields = len(User.model_fields)
        populated = sum(1 for v in [partial_user.name, partial_user.age, partial_user.bio] if v is not None)
        completion = int(populated / total_fields * 100)
        
        # Build a dictionary of only the fields that have values
        data = {}
        if partial_user.name is not None:
            data["name"] = partial_user.name
        if partial_user.age is not None:
            data["age"] = partial_user.age
        if partial_user.bio is not None:
            data["bio"] = partial_user.bio

        self.progress = {
            "completion": f"{completion}%",
            "data": data
        }

        return self.progress

def stream_with_progress():
    tracker = ProgressTracker()

    user_stream = client.chat.completions.create_partial(
        model="gpt-3.5-turbo",
        response_model=User,
        messages=[
            {"role": "user", "content": "Generate a profile for a fictional user named Carol who is 42 years old."}
        ]
    )

    for partial_user in user_stream:
        progress = tracker.update(partial_user)
        print(f"Progress: {progress['completion']} - Current data: {progress['data']}")
import asyncio
from openai import AsyncOpenAI
async def stream_async():
    async_client = instructor.from_openai(AsyncOpenAI())

    # Use async/await pattern for non-blocking streaming
    user_stream = await async_client.chat.completions.create_partial(
        model="gpt-3.5-turbo",
        response_model=User,
        messages=[
            {"role": "user", "content": "Generate a profile for a fictional user named Dave who is 31 years old."}
        ]
    )

    # Process stream with async for loop
    async for partial_user in user_stream:
        print(f"Async stream update: {partial_user}")
asyncio.run(stream_async())
```

```shell
$ pip install instructor pydantic openai
$ python streaming-basics.py
```

For more information, see the original documentation:
- https://github.com/jxnl/instructor
- https://instructor-ai.github.io/instructor/concepts/streaming/

### Streaming Lists

```python
from pydantic import BaseModel, Field
from typing import List
import instructor
from openai import OpenAI

class Person(BaseModel):
    name: str
    age: int
    occupation: str
client = instructor.from_openai(OpenAI())
people_stream = client.chat.completions.create_iterable(
    model="gpt-3.5-turbo",
    response_model=Person,  # Note: no List[] wrapper needed here
    messages=[
        {"role": "user", "content": """
            Generate profiles for three different people:
            1. A software engineer in their 30s
            2. A teacher in their 40s
            3. A doctor in their 50s
        """}
    ]
)
print("Receiving people one at a time:")
for i, person in enumerate(people_stream, 1):
    print(f"\nPerson {i}:")
    print(f"Name: {person.name}")
    print(f"Age: {person.age}")
    print(f"Occupation: {person.occupation}")
    # Note: Each person is fully complete when received
from pydantic import BaseModel, Field
from typing import List, Optional

class Book(BaseModel):
    title: str
    author: str
    year: int
    genre: str
    summary: str = Field(description="Brief summary of the book's plot")
    rating: Optional[float] = Field(None, ge=0, le=5, description="Rating from 0-5 stars")
books_stream = client.chat.completions.create_iterable(
    model="gpt-3.5-turbo",
    response_model=Book,
    messages=[
        {"role": "system", "content": "Generate detailed book entries with accurate information."},
        {"role": "user", "content": """
            Generate entries for three classic science fiction books.
            Include their titles, authors, publication years, and summaries.
        """}
    ]
)
print("Streaming book data:")
for i, book in enumerate(books_stream, 1):
    print(f"\nBook {i}: {book.title} ({book.year})")
    print(f"Author: {book.author}")
    print(f"Genre: {book.genre}")
    print(f"Rating: {book.rating if book.rating is not None else 'Not rated'}")
    print(f"Summary: {book.summary}")

from typing import List, Dict, Any
import time

class Task(BaseModel):
    title: str
    priority: str
    estimated_hours: float
    assigned_to: Optional[str] = None
all_tasks = []
total_hours = 0
by_priority = {"high": 0, "medium": 0, "low": 0}
by_assignee = {}
tasks_stream = client.chat.completions.create_iterable(
    model="gpt-3.5-turbo",
    response_model=Task,
    messages=[
        {"role": "user", "content": """
            Generate 5 tasks for a software development sprint.
            Include high, medium, and low priority tasks.
            Assign team members: Alex, Jamie, Taylor, and Morgan.
        """}
    ]
)
print("Project task planning:")
print("---------------------")

for task in tasks_stream:
    # Update statistics
    all_tasks.append(task)
    total_hours += task.estimated_hours
    by_priority[task.priority.lower()] += 1

    if task.assigned_to:
        by_assignee[task.assigned_to] = by_assignee.get(task.assigned_to, 0) + 1

    # Print the task
    print(f"\nNew Task: {task.title}")
    print(f"Priority: {task.priority}")
    print(f"Estimate: {task.estimated_hours} hours")
    print(f"Assigned to: {task.assigned_to or 'Unassigned'}")

    # Print current statistics
    print("\nCurrent Sprint Stats:")
    print(f"Tasks planned: {len(all_tasks)}")
    print(f"Total hours: {total_hours:.1f}")
    print(f"By priority: {by_priority}")
    print(f"By assignee: {by_assignee}")

    # Simulate a pause for real-time updates
    time.sleep(0.5)

print("\nSprint planning complete!")

from typing import Dict, List, Any, Generator, TypeVar, Generic

T = TypeVar('T')

def combine_streams(streams: Dict[str, Generator[T, None, None]]) -> Generator[Dict[str, T], None, None]:
    """Combine multiple iterables with identification."""
    active_streams = streams.copy()
    results = {key: None for key in streams}

    while active_streams:
        for key, stream in list(active_streams.items()):
            try:
                value = next(stream)
                results[key] = value
                yield results.copy()
            except StopIteration:
                del active_streams[key]
class DocumentSummary(BaseModel):
    title: str
    content_type: str
    key_points: List[str]
    word_count: int
prompts = {
    "emails": "Generate summaries for 3 important emails about project deadlines",
    "reports": "Generate summaries for 2 financial reports about quarterly earnings",
    "articles": "Generate summaries for 2 news articles about technology trends"
}
streams = {}
for category, prompt in prompts.items():
    streams[category] = client.chat.completions.create_iterable(
        model="gpt-3.5-turbo",
        response_model=DocumentSummary,
        messages=[{"role": "user", "content": prompt}]
    )
for i, result in enumerate(combine_streams(streams), 1):
    print(f"\nUpdate {i}:")
    for category, doc in result.items():
        if doc:
            print(f"  {category.upper()}: {doc.title}")
        else:
            print(f"  {category.upper()}: No documents yet")

from typing import List, Optional, Iterator
import itertools

class NewsHeadline(BaseModel):
    title: str
    source: str
    category: str
    publish_date: str
    summary: str
headlines_stream = client.chat.completions.create_iterable(
    model="gpt-3.5-turbo",
    response_model=NewsHeadline,
    messages=[
        {"role": "user", "content": "Generate 10 fictional technology news headlines from the past week."}
    ]
)
print("Top Headlines:")
for i, headline in enumerate(itertools.islice(headlines_stream, 3)):
    print(f"\nHeadline {i+1}: {headline.title}")
    print(f"Source: {headline.source}")
    print(f"Category: {headline.category}")
    print(f"Date: {headline.publish_date}")
    print(f"Summary: {headline.summary}")
print("\nShowing only the first 3 headlines.")
```

```shell
$ pip install instructor pydantic
$ python streaming-lists.py
```

For more information, see the original documentation:
- https://github.com/jxnl/instructor

## Advanced Structures

Building complex structured outputs

### Recursive Structures

```python
import instructor
from openai import OpenAI
import enum
from pydantic import BaseModel, Field
client = instructor.from_openai(OpenAI())
class NodeType(str, enum.Enum):
    FILE = "file"
    FOLDER = "folder"
class Node(BaseModel):
    name: str = Field(..., description="Name of the node")
    children: list["Node"] = Field(
        default_factory=list,
        description="List of children nodes, only applicable for folders"
    )
    node_type: NodeType = Field(
        default=NodeType.FILE,
        description="Either a file or folder"
    )
Node.model_rebuild()
class DirectoryTree(BaseModel):
    root: Node = Field(..., description="Root folder of the directory tree")
def parse_directory_structure(text_representation: str) -> DirectoryTree:
    return client.chat.completions.create(
        model="gpt-3.5-turbo",
        response_model=DirectoryTree,
        messages=[
            {
                "role": "system",
                "content": "Parse the following directory structure into a tree."
            },
            {
                "role": "user",
                "content": f"Parse this directory structure:\n{text_representation}"
            }
        ]
    )
directory_structure = '''
root
├── images
│   ├── logo.png
│   └── banner.jpg
└── docs
    ├── readme.md
    └── config
        └── settings.json
'''

result = parse_directory_structure(directory_structure)
```

```shell
$ pip install instructor pydantic
$ python recursive-structures.py
```

For more information, see the original documentation:
- https://github.com/jxnl/instructor

### Knowledge Graphs

```python
import instructor
from openai import OpenAI
from pydantic import BaseModel, Field
client = instructor.from_openai(OpenAI())
class Node(BaseModel):
    id: int
    label: str
    color: str
class Edge(BaseModel):
    source: int
    target: int
    label: str
    color: str = "black"
class KnowledgeGraph(BaseModel):
    nodes: list[Node] = Field(..., default_factory=list)
    edges: list[Edge] = Field(..., default_factory=list)
def generate_knowledge_graph(input_text: str) -> KnowledgeGraph:
    return client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {
                "role": "user",
                "content": f"Create a detailed knowledge graph for: {input_text}"
            }
        ],
        response_model=KnowledgeGraph
    )
graph = generate_knowledge_graph("Quantum mechanics and its applications")
for node in graph.nodes:
    print(f"Node {node.id}: {node.label} ({node.color})")

for edge in graph.edges:
    print(f"Edge: {edge.source} --({edge.label})--> {edge.target}")
from graphviz import Digraph

def visualize_knowledge_graph(kg: KnowledgeGraph):
    dot = Digraph(comment="Knowledge Graph")

    # Add nodes
    for node in kg.nodes:
        dot.node(str(node.id), node.label, color=node.color)

    # Add edges
    for edge in kg.edges:
        dot.edge(str(edge.source), str(edge.target),
                 label=edge.label, color=edge.color)

    # Render the graph
    dot.render("knowledge_graph.gv", view=True)
visualize_knowledge_graph(graph)
```

```shell
$ pip install instructor pydantic
$ python knowledge-graphs.py
```

For more information, see the original documentation:
- https://github.com/jxnl/instructor

### Dependency Trees

```python
import instructor
from openai import OpenAI
from pydantic import BaseModel, Field
client = instructor.from_openai(OpenAI())
class DependencyNode(BaseModel):
    id: str
    description: str
    dependencies: list[str] = Field(default_factory=list,
                                   description="IDs of nodes this node depends on")
class DependencyTree(BaseModel):
    nodes: list[DependencyNode]

    def get_execution_order(self) -> list[str]:
        """Returns topologically sorted execution order."""
        # Build dependency graph
        dep_graph = {node.id: set(node.dependencies) for node in self.nodes}
        result = []

        # Find nodes with no dependencies
        while dep_graph:
            # Find nodes with no dependencies
            roots = {node for node, deps in dep_graph.items() if not deps}
            if not roots:
                raise ValueError("Circular dependency detected")

            # Add these nodes to the result
            result.extend(sorted(roots))

            # Remove these nodes from the graph
            dep_graph = {
                node: (deps - roots)
                for node, deps in dep_graph.items()
                if node not in roots
            }

        return result
def extract_dependencies(project_description: str) -> DependencyTree:
    return client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {
                "role": "system",
                "content": "Extract the dependencies between tasks in this project."
            },
            {
                "role": "user",
                "content": project_description
            }
        ],
        response_model=DependencyTree
    )
project = """
Building a web application requires:
1. Setup development environment
2. Design database schema (after setup)
3. Create API endpoints (after database schema)
4. Build frontend UI (after API design)
5. Write tests (after API and UI)
6. Deploy application (after tests pass)
"""

dependencies = extract_dependencies(project)
execution_order = dependencies.get_execution_order()
print("Execution order:", execution_order)
```

```shell
$ pip install instructor pydantic
$ python dependency-trees.py
```

For more information, see the original documentation:
- https://github.com/jxnl/instructor

### Task Planning

```python
import asyncio
import instructor
from openai import OpenAI
from pydantic import BaseModel, Field
client = instructor.from_openai(OpenAI())
class TaskResult(BaseModel):
    task_id: int
    result: str

class TaskResults(BaseModel):
    results: list[TaskResult]
class Task(BaseModel):
    id: int = Field(..., description="Unique id of the task")
    task: str = Field(..., description="The task to be performed")
    subtasks: list[int] = Field(
        default_factory=list,
        description="IDs of subtasks that must be completed before this task"
    )
async def execute(self, with_results: TaskResults) -> TaskResult:
        """Execute this task and return the result."""
        return TaskResult(task_id=self.id, result=f"Result for task: {self.task}")
class TaskPlan(BaseModel):
    task_graph: list[Task] = Field(
        ...,
        description="List of tasks and their dependencies"
    )

    def _get_execution_order(self) -> list[int]:
        """Compute topological sort of tasks based on dependencies."""
        dep_graph = {task.id: set(task.subtasks) for task in self.task_graph}
        result = []
while dep_graph:
            available = {task_id for task_id, deps in dep_graph.items() if not deps}  # Tasks with no dependencies
            if not available:
                raise ValueError("Circular dependency detected in tasks")

            result.extend(sorted(available))  # Add to execution order

            # Update dependency graph by removing completed tasks
            dep_graph = {
                task_id: (deps - available)
                for task_id, deps in dep_graph.items()
                if task_id not in available
            }

        return result

    async def execute(self) -> dict[int, TaskResult]:
        """Execute all tasks in dependency order."""
        execution_order = self._get_execution_order()
        tasks_by_id = {task.id: task for task in self.task_graph}
        results = {}
while len(results) < len(self.task_graph):
            # Identify tasks whose dependencies are all satisfied
            ready_tasks = [
                tasks_by_id[task_id]
                for task_id in execution_order
                if task_id not in results and
                all(dep_id in results for dep_id in tasks_by_id[task_id].subtasks)
            ]

            # Process all ready tasks concurrently
            new_results = await asyncio.gather(*[
                task.execute(
                    with_results=TaskResults(
                        results=[
                            results[dep_id]
                            for dep_id in task.subtasks
                        ]
                    )
                )
                for task in ready_tasks
            ])

            # Save results for dependent tasks to use
            for result in new_results:
                results[result.task_id] = result

        return results
def create_task_plan(question: str) -> TaskPlan:
    return client.chat.completions.create(
        model="gpt-4",
        messages=[
            {
                "role": "system",
                "content": "Create a detailed task plan to answer the user's question. Break down the problem into smaller, dependent tasks."
            },
            {
                "role": "user",
                "content": question
            }
        ],
        response_model=TaskPlan
    )
async def main():
    plan = create_task_plan(
        "What is the economic impact of renewable energy adoption in developing countries?"
    )
    print("Task Plan:")
    for task in plan.task_graph:
        deps = f" (depends on: {task.subtasks})" if task.subtasks else ""
        print(f"Task {task.id}: {task.task}{deps}")

    print("\nExecuting plan...")
    results = await plan.execute()

    print("\nResults:")
    for task_id, result in sorted(results.items()):
        print(f"Task {task_id}: {result.result}")
if __name__ == "__main__":
    asyncio.run(main())
```

```shell
$ pip install instructor pydantic
$ python task-planning.py
```

For more information, see the original documentation:
- https://github.com/jxnl/instructor

### Document Structure

```python
import instructor
from openai import OpenAI
from pydantic import BaseModel, Field
from typing import Optional, List
client = instructor.from_openai(OpenAI())
class Section(BaseModel):
    heading: str
    content: str
    subsections: List["Section"] = Field(default_factory=list)
Section.model_rebuild()
class Document(BaseModel):
    title: str
    abstract: Optional[str] = None
    authors: List[str] = Field(default_factory=list)
    sections: List[Section] = Field(default_factory=list)
    keywords: List[str] = Field(default_factory=list)
def extract_document_structure(text: str) -> Document:
    return client.chat.completions.create(
        model="gpt-4",
        messages=[
            {
                "role": "system",
                "content": "Extract the structured representation of this document, including all sections and subsections."
            },
            {
                "role": "user",
                "content": text
            }
        ],
        response_model=Document
    )
document_text = """
This paper explores the applications of machine learning in healthcare settings.
machine learning, healthcare, AI, medical diagnosis
Machine learning has shown promising results in healthcare applications.
Healthcare has historically been slow to adopt new technologies.
Data privacy and model interpretability remain significant challenges.
We employed a mixed-methods approach.
Our findings indicate a 30% improvement in diagnostic accuracy.
These results have significant implications for clinical practice.
Machine learning will continue to transform healthcare delivery.
"""

doc_structure = extract_document_structure(document_text)
print(f"Title: {doc_structure.title}")
print(f"Authors: {', '.join(doc_structure.authors)}")
if doc_structure.abstract:
    print(f"Abstract: {doc_structure.abstract}")
print(f"Keywords: {', '.join(doc_structure.keywords)}")
print("\nSections:")
for section in doc_structure.sections:
    print(f"- {section.heading}")
    for subsection in section.subsections:
        print(f"  - {subsection.heading}")
```

```shell
$ pip install instructor pydantic
$ python document-structure.py
```

For more information, see the original documentation:
- https://github.com/jxnl/instructor

## Validation

Ensuring data quality with validation

### Validation Basics

```python
import instructor
from openai import OpenAI
from pydantic import BaseModel, Field
client = instructor.from_openai(OpenAI())
class User(BaseModel):
    name: str = Field(..., description="User's full name")
    age: int = Field(...,
                    description="User's age in years",
                    ge=0, le=120)  # Must be between 0 and 120
    email: str = Field(..., description="User's email address")
def extract_user(text: str) -> User:
    return client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {
                "role": "user",
                "content": f"Extract user information from this text: {text}"
            }
        ],
        response_model=User
    )
text = "John Doe is 25 years old and his email is john.doe@example.com."
user = extract_user(text)
print(user.model_dump_json(indent=2))
import instructor
from openai import OpenAI
from pydantic import BaseModel, Field, field_validator
client = instructor.from_openai(OpenAI())
class User(BaseModel):
    name: str
    age: int

    @field_validator("age")
    def validate_age(cls, v):
        if v < 0 or v > 120:
            raise ValueError("Age must be between 0 and 120")
        return v
user = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
        {
            "role": "user",
            "content": "Extract: John Doe, age: 150"
        }
    ],
    response_model=User,
    max_retries=2  # Try up to 2 more times if validation fails
)

print(user.model_dump_json(indent=2))
```

```shell
$ pip install instructor pydantic
$ python validation-basics.py
```

For more information, see the original documentation:
- https://github.com/jxnl/instructor

### Custom Validators

```python
import instructor
from openai import OpenAI
from pydantic import BaseModel, Field, field_validator
client = instructor.from_openai(OpenAI())
class Contact(BaseModel):
    name: str = Field(description="Person's full name")
    email: str = Field(description="Person's email address")
    phone: str = Field(description="Person's phone number")

    @field_validator("email")
    def validate_email(cls, v):
        if "@" not in v:
            raise ValueError("Email must contain @ symbol")
        return v

    @field_validator("phone")
    def validate_phone(cls, v):
        # Remove all non-numeric characters
        digits = ''.join(c for c in v if c.isdigit())
        if len(digits) < 10:
            raise ValueError("Phone number must have at least 10 digits")
        return v
contact = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
        {
            "role": "user",
            "content": "Extract: John Doe, email: johndoe.example.com, phone: 555-1234"
        }
    ],
    response_model=Contact,
    max_retries=2
)

print(contact.model_dump_json(indent=2))
from typing_extensions import Annotated
from pydantic import AfterValidator
def validate_uppercase(v: str) -> str:
    if v != v.upper():
        raise ValueError("String must be uppercase")
    return v
class Document(BaseModel):
    title: Annotated[str, AfterValidator(validate_uppercase)]
    content: str

    @field_validator("content")
    def validate_content_length(cls, v):
        if len(v.split()) < 5:
            raise ValueError("Content must be at least 5 words long")
        return v
import instructor
from openai import OpenAI
from pydantic import BaseModel, BeforeValidator
from typing_extensions import Annotated
from instructor import llm_validator
client = instructor.from_openai(OpenAI())
class Review(BaseModel):
    product: str
    content: Annotated[
        str,
        BeforeValidator(llm_validator("must be positive and respectful", client=client))
    ]
    rating: int
review = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
        {
            "role": "user",
            "content": "Extract: iPhone 14, content: This product is terrible and I hate it, rating: 1"
        }
    ],
    response_model=Review,
    max_retries=2
)
```

```shell
$ pip install instructor pydantic
$ python custom-validators.py
```

For more information, see the original documentation:
- https://github.com/jxnl/instructor

### Retry Mechanisms

```python
import instructor
from openai import OpenAI
from pydantic import BaseModel, Field, field_validator
client = instructor.from_openai(OpenAI())
class Profile(BaseModel):
    username: str = Field(description="Username without spaces")
    age: int = Field(description="Age in years", ge=13)

    @field_validator("username")
    def validate_username(cls, v):
        if " " in v:
            raise ValueError("Username cannot contain spaces")
        return v
profile = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
        {
            "role": "user",
            "content": "Extract: username: John Doe, age: 25"
        }
    ],
    response_model=Profile,
    max_retries=3  # Try up to 3 more times if validation fails
)

print(profile.model_dump_json(indent=2))
import instructor
from openai import OpenAI
from pydantic import BaseModel, field_validator
from tenacity import Retrying, stop_after_attempt, wait_fixed
client = instructor.from_openai(OpenAI())
class User(BaseModel):
    name: str
    age: int

    @field_validator("name")
    def validate_name(cls, v):
        if not v.isupper():
            raise ValueError("Name must be uppercase")
        return v
user = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
        {
            "role": "user",
            "content": "Extract: John is 30 years old"
        }
    ],
    response_model=User,
    max_retries=Retrying(
        stop=stop_after_attempt(3),  # Stop after 3 attempts
        wait=wait_fixed(1),  # Wait 1 second between attempts
    )
)

print(user.model_dump_json(indent=2))
import instructor
from openai import OpenAI
from pydantic import BaseModel, field_validator
from instructor.exceptions import InstructorRetryException
client = instructor.from_openai(OpenAI())
class ImpossibleModel(BaseModel):
    name: str
    age: int

    @field_validator("age")
    def validate_age(cls, v):
        raise ValueError("This validator will always fail")
try:
    result = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {
                "role": "user",
                "content": "Extract: Jane is 25 years old"
            }
        ],
        response_model=ImpossibleModel,
        max_retries=2
    )
except InstructorRetryException as e:
    print(f"Failed after {e.n_attempts} attempts")
    print(f"Last error message: {e.messages[-1]['content']}")

    # Implement fallback strategy here
    fallback_result = {"name": "Jane", "age": 0}
    print(f"Using fallback: {fallback_result}")
```

```shell
$ pip install instructor pydantic
$ python retry-mechanisms.py
```

For more information, see the original documentation:
- https://github.com/jxnl/instructor

### Fallback Strategies

```python
import instructor
from openai import OpenAI
from pydantic import BaseModel, Field, ValidationError
from instructor.exceptions import InstructorRetryException
client = instructor.from_openai(OpenAI())
class DetailedUserProfile(BaseModel):
    name: str = Field(description="User's full name")
    age: int = Field(description="User's age in years", ge=18)
    occupation: str = Field(description="User's job or profession")
    income: int = Field(description="User's annual income in USD", ge=0)
    education: str = Field(description="User's highest education level")
class BasicUserProfile(BaseModel):
    name: str = Field(description="User's name")
    age: int = Field(description="User's age", ge=0)
def extract_user_with_fallback(text: str):
    try:
        return client.chat.completions.create(  # First attempt with detailed model
            model="gpt-3.5-turbo",
            messages=[
                {
                    "role": "user",
                    "content": f"Extract user information: {text}"
                }
            ],
            response_model=DetailedUserProfile,
            max_retries=2
        )
    except InstructorRetryException:
        print("Detailed extraction failed, falling back to basic profile")
        return client.chat.completions.create(  # Fall back to simpler model
            model="gpt-3.5-turbo",
            messages=[
                {
                    "role": "user",
                    "content": f"Extract basic user information: {text}"
                }
            ],
            response_model=BasicUserProfile,
            max_retries=1
        )
text = "John is 25 years old"
user = extract_user_with_fallback(text)
print(user.model_dump_json(indent=2))
from typing import Optional
import instructor
from openai import OpenAI
from pydantic import BaseModel, Field
client = instructor.from_openai(OpenAI())
class FlexibleProfile(BaseModel):
    name: str = Field(description="Person's name")
    age: Optional[int] = Field(None, description="Person's age if mentioned")
    location: Optional[str] = Field(None, description="Person's location if mentioned")
    occupation: Optional[str] = Field(None, description="Person's job if mentioned")
profile = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
        {
            "role": "user",
            "content": "Sarah is a software engineer from Boston"
        }
    ],
    response_model=FlexibleProfile
)

print(profile.model_dump_json(indent=2))
import instructor
from openai import OpenAI
from pydantic import BaseModel, Field, ValidationError
from enum import Enum
client = instructor.from_openai(OpenAI())
class ExtractionStatus(str, Enum):
    SUCCESS = "success"
    PARTIAL = "partial"
    FAILED = "failed"
class Contact(BaseModel):
    name: str
    email: str
    phone: str
class ExtractionResult(BaseModel):
    status: ExtractionStatus
    data: dict
    error_message: str = ""
def extract_with_robustness(text: str) -> ExtractionResult:
    try:
        result = client.chat.completions.create(  # Primary extraction attempt
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": f"Extract contact info: {text}"}],
            response_model=Contact,
            max_retries=2
        )
        return ExtractionResult(
            status=ExtractionStatus.SUCCESS,
            data=result.model_dump()
        )
except InstructorRetryException as e:
        try:
            partial_data = {}
            error_msg = e.messages[-1]["content"]  # Parse the error message
            text_lines = text.split('\n')
            for line in text_lines:
                if "name:" in line.lower():
                    partial_data["name"] = line.split("name:")[1].strip()
                if "email:" in line.lower():
                    partial_data["email"] = line.split("email:")[1].strip()
                if "phone:" in line.lower():
                    partial_data["phone"] = line.split("phone:")[1].strip()

            if partial_data:
                return ExtractionResult(
                    status=ExtractionStatus.PARTIAL,
                    data=partial_data,
                    error_message=error_msg
                )
            else:
                return ExtractionResult(
                    status=ExtractionStatus.FAILED,
                    data={},
                    error_message=error_msg
                )
        except Exception as nested_error:
            return ExtractionResult(
                status=ExtractionStatus.FAILED,
                data={},
                error_message=f"Complete extraction failure: {str(nested_error)}"
            )
```

```shell
$ pip install instructor pydantic
$ python fallback-strategies.py
```

For more information, see the original documentation:
- https://github.com/jxnl/instructor

### Field-level Validation

```python
import instructor
from openai import OpenAI
from pydantic import BaseModel, Field, field_validator
import re
client = instructor.from_openai(OpenAI())
class UserProfile(BaseModel):
    username: str = Field(
        description="Username (lowercase, no spaces)",
        min_length=3,
        max_length=20
    )
    email: str = Field(
        description="Valid email address"
    )
    age: int = Field(
        description="Age in years",
        ge=13,  # Greater than or equal to 13
        le=120  # Less than or equal to 120
    )

    @field_validator("username")
    def validate_username(cls, v):
        if not v.islower() or " " in v:
            raise ValueError("Username must be lowercase and contain no spaces")
        return v

    @field_validator("email")
    def validate_email(cls, v):
        pattern = r"^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+$"
        if not re.match(pattern, v):
            raise ValueError("Invalid email format")
        return v
profile = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
        {
            "role": "user",
            "content": "Extract user profile: John Doe, john.doe@example.com, 25 years old"
        }
    ],
    response_model=UserProfile,
    max_retries=2
)

print(profile.model_dump_json(indent=2))
import instructor
from openai import OpenAI
from pydantic import BaseModel, Field
from typing import List
client = instructor.from_openai(OpenAI())
class Product(BaseModel):
    name: str = Field(
        description="Product name",
        min_length=2,
        max_length=100
    )
    price: float = Field(
        description="Product price in USD",
        gt=0  # Greater than 0
    )
    description: str = Field(
        description="Product description",
        min_length=10,
        max_length=1000
    )
    tags: List[str] = Field(
        description="Product tags",
        min_length=1,  # At least one tag
        max_length=10  # At most 10 tags
    )
product = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
        {
            "role": "user",
            "content": "Extract product info: iPhone, $999, Latest smartphone with advanced features, tags: electronics, smartphone, apple"
        }
    ],
    response_model=Product
)

print(product.model_dump_json(indent=2))
from typing_extensions import Annotated
from pydantic import AfterValidator
import re

def validate_phone_number(v: str) -> str:
    """Validate phone number format."""
    # Remove all non-numeric characters
    digits = ''.join(c for c in v if c.isdigit())
    if len(digits) < 10:
        raise ValueError("Phone number must have at least 10 digits")
    return v

def validate_zip_code(v: str) -> str:
    """Validate US zip code format."""
    pattern = r"^\d{5}(-\d{4})?$"
    if not re.match(pattern, v):
        raise ValueError("Invalid zip code format (must be 12345 or 12345-6789)")
    return v

class Address(BaseModel):
    street: str
    city: str
    state: str
    zip_code: Annotated[str, AfterValidator(validate_zip_code)]
    phone: Annotated[str, AfterValidator(validate_phone_number)]
```

```shell
$ pip install instructor pydantic
$ python field-level-validation.py
```

For more information, see the original documentation:
- https://github.com/jxnl/instructor

## Performance and Optimization

Optimizing performance for production use

### Caching Responses

```python
import functools
import instructor
from openai import OpenAI
from pydantic import BaseModel
client = instructor.from_openai(OpenAI())
class User(BaseModel):
    name: str
    age: int
@functools.cache
def extract_user(text: str) -> User:
    """Extract user information with in-memory caching."""
    return client.chat.completions.create(
        model="gpt-3.5-turbo",
        response_model=User,
        messages=[
            {
                "role": "user",
                "content": f"Extract user information from: {text}"
            }
        ]
    )
user1 = extract_user("John is 30 years old")
print(user1)
user2 = extract_user("John is 30 years old")
print(user2)
import functools
import inspect
import instructor
import diskcache
from openai import OpenAI
from pydantic import BaseModel
client = instructor.from_openai(OpenAI())
cache = diskcache.Cache('./my_cache_directory')
def instructor_cache(func):
    """Cache a function that returns a Pydantic model."""
    return_type = inspect.signature(func).return_annotation
    if not issubclass(return_type, BaseModel):
        raise ValueError("The return type must be a Pydantic model")

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        # Create a cache key from the function name and arguments
        key = f"{func.__name__}-{functools._make_key(args, kwargs, typed=False)}"

        # Check if result is already cached
        if (cached := cache.get(key)) is not None:
            # Deserialize from JSON based on the return type
            return return_type.model_validate_json(cached)

        # Call the function and cache its result
        result = func(*args, **kwargs)
        serialized_result = result.model_dump_json()
        cache.set(key, serialized_result)

        return result

    return wrapper
class Product(BaseModel):
    name: str
    price: float
    category: str
@instructor_cache
def extract_product(text: str) -> Product:
    """Extract product information with disk caching."""
    return client.chat.completions.create(
        model="gpt-3.5-turbo",
        response_model=Product,
        messages=[
            {
                "role": "user",
                "content": f"Extract product information from: {text}"
            }
        ]
    )
product = extract_product("iPhone 14 Pro costs $999 and is in the smartphones category")
print(product)
import redis
import functools
import inspect
import instructor
from openai import OpenAI
from pydantic import BaseModel
client = instructor.from_openai(OpenAI())
cache = redis.Redis(host="localhost", port=6379, db=0)
def redis_cache(func):
    """Cache a function that returns a Pydantic model using Redis."""
    return_type = inspect.signature(func).return_annotation
    if not issubclass(return_type, BaseModel):
        raise ValueError("The return type must be a Pydantic model")

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        # Create a cache key from the function name and arguments
        key = f"{func.__name__}-{functools._make_key(args, kwargs, typed=False)}"

        # Check if result is already cached
        if (cached := cache.get(key)) is not None:
            # Deserialize from JSON based on the return type
            return return_type.model_validate_json(cached)

        # Call the function and cache its result
        result = func(*args, **kwargs)
        serialized_result = result.model_dump_json()
        cache.set(key, serialized_result)

        return result

    return wrapper
```

```shell
$ pip install instructor pydantic
$ python caching-responses.py
```

For more information, see the original documentation:
- https://github.com/jxnl/instructor

### Parallel Extraction

Process multiple extractions in parallel with Instructor for improved efficiency.

Problem:
Traditional extraction methods process one piece of information at a time, causing increased latency
and inefficient use of context window when dealing with multiple related extractions.

Solution:
Instructor's parallel mode enables simultaneous extraction of multiple structured objects,
improving response time and making better use of the model's capabilities.

```python
import instructor
from openai import OpenAI
from typing import Iterable, Literal, Union, List
from pydantic import BaseModel, Field
client = instructor.from_openai(OpenAI(), mode=instructor.Mode.PARALLEL_TOOLS)
class Weather(BaseModel):
    location: str
    units: Literal["imperial", "metric"]

class SearchQuery(BaseModel):
    query: str
def extract_parallel_info(user_query: str) -> list[Union[Weather, SearchQuery]]:
    """
    Extract both weather requests and search queries from a single user input.
    
    This allows the model to process multiple intents in one go, such as checking
    weather for multiple locations while also performing searches.
    """
    function_calls = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "You must always use tools"},
            {
                "role": "user",
                "content": user_query
            }
        ],
        response_model=Iterable[Weather | SearchQuery]
    )

    return list(function_calls)  # Convert the iterable to a list
def demonstrate_basic_parallel():
    results = extract_parallel_info(
        "What's the weather in New York and Tokyo? Also, find information about renewable energy."
    )

    for result in results:
        if isinstance(result, Weather):
            print(f"Weather request for {result.location} in {result.units} units")
        elif isinstance(result, SearchQuery):
            print(f"Search query: {result.query}")
class Person(BaseModel):
    name: str
    age: int
    occupation: str

class Company(BaseModel):
    name: str
    industry: str
    year_founded: int

class Location(BaseModel):
    city: str
    country: str
    population: int = Field(description="Approximate population")
def extract_entities(text: str) -> List[Union[Person, Company, Location]]:
    """
    Extract multiple entity types from a text simultaneously.
    
    This function identifies and structures information about people,
    companies, and locations from unstructured text in a single pass.
    """
    results = client.chat.completions.create(
        model="gpt-4-turbo",
        messages=[
            {"role": "system", "content": "Extract all relevant entities from the text."},
            {"role": "user", "content": text}
        ],
        response_model=Iterable[Person | Company | Location]
    )

    return list(results)

def demonstrate_entity_extraction():
    # Sample text containing multiple entity types
    text = """
    John Smith is a 35-year-old software engineer living in San Francisco, USA,
    a city with about 815,000 people. He works at TechCorp, a software development
    company founded in 2005 that specializes in AI applications. His colleague
    Maria Rodriguez, 29, is a data scientist who recently moved from Madrid, Spain,
    a city of approximately 3.2 million people.
    """

    entities = extract_entities(text)

    # Process different entity types
    people = [e for e in entities if isinstance(e, Person)]
    companies = [e for e in entities if isinstance(e, Company)]
    locations = [e for e in entities if isinstance(e, Location)]

    print(f"Found {len(people)} people, {len(companies)} companies, and {len(locations)} locations")
    
    # Display extracted entities
    for person in people:
        print(f"Person: {person.name}, {person.age}, {person.occupation}")
    
    for company in companies:
        print(f"Company: {company.name}, {company.industry}, founded in {company.year_founded}")
    
    for location in locations:
        print(f"Location: {location.city}, {location.country}, pop. {location.population}")
if __name__ == "__main__":
    print("\n--- Basic Parallel Extraction ---")
    demonstrate_basic_parallel()
    
    print("\n--- Advanced Entity Extraction ---")
    demonstrate_entity_extraction()
```

```shell
$ pip install instructor pydantic
$ python parallel-extraction.py
```

For more information, see the original documentation:
- https://github.com/jxnl/instructor

### Batch Processing

```python
import asyncio
import instructor
from openai import AsyncOpenAI
from pydantic import BaseModel, Field
from typing import List, Tuple
client = instructor.from_openai(AsyncOpenAI())
sem = asyncio.Semaphore(5)
class SentimentAnalysis(BaseModel):
    sentiment: str = Field(description="The sentiment of the text (positive, negative, or neutral)")
    confidence: float = Field(description="Confidence score from 0.0 to 1.0")
    reasoning: str = Field(description="Brief explanation for the sentiment classification")
async def analyze_sentiment(text: str) -> Tuple[str, SentimentAnalysis]:
    async with sem:  # Rate limiting
        result = await client.chat.completions.create(
            model="gpt-3.5-turbo",
            response_model=SentimentAnalysis,
            messages=[
                {
                    "role": "user",
                    "content": f"Analyze the sentiment of this text: {text}"
                }
            ]
        )
        return text, result
async def process_batch(texts: List[str]):
    tasks = [analyze_sentiment(text) for text in texts]  # Create tasks for all texts

    # Collect results as tasks complete (in any order)
    results = []
    for task in asyncio.as_completed(tasks):
        original_text, sentiment = await task
        results.append({
            "text": original_text,
            "sentiment": sentiment.sentiment,
            "confidence": sentiment.confidence,
            "reasoning": sentiment.reasoning
        })

    return results
async def main():
    texts = [
        "I absolutely love this product! It's amazing.",
        "The service was terrible and the staff was rude.",
        "The weather is cloudy today with a chance of rain.",
        "I'm disappointed with the quality of this item.",
        "The conference was informative and well-organized."
    ]

    results = await process_batch(texts)

    for result in results:
        print(f"Text: {result['text']}")
        print(f"Sentiment: {result['sentiment']} (Confidence: {result['confidence']:.2f})")
        print(f"Reasoning: {result['reasoning']}")
        print("-" * 50)
if __name__ == "__main__":
    asyncio.run(main())
import json
import asyncio
import instructor
from openai import AsyncOpenAI
from pydantic import BaseModel, Field
from enum import Enum
from typing import List, Dict, Any, Optional
client = instructor.from_openai(AsyncOpenAI())
class Category(str, Enum):
    PRODUCT = "PRODUCT"
    SERVICE = "SERVICE"
    FEATURE = "FEATURE"
    SUPPORT = "SUPPORT"
    OTHER = "OTHER"

class FeedbackClassification(BaseModel):
    categories: List[Category] = Field(description="Categories that apply to this feedback")
    priority: int = Field(description="Priority score from 1-5, where 5 is highest priority", ge=1, le=5)
    analysis: str = Field(description="Brief analysis of the feedback")
async def process_item(item: str, retry_count: int = 2) -> Dict[str, Any]:
    attempts = 0
    while attempts <= retry_count:
        try:
            result = await client.chat.completions.create(
                model="gpt-3.5-turbo",
                response_model=FeedbackClassification,
                messages=[
                    {
                        "role": "system",
                        "content": "You are a customer feedback analyzer. Categorize and prioritize the feedback."
                    },
                    {
                        "role": "user",
                        "content": f"Analyze this feedback: {item}"
                    }
                ]
            )
            return {
                "feedback": item,
                "categories": [c.value for c in result.categories],
                "priority": result.priority,
                "analysis": result.analysis,
                "status": "success"
            }
        except Exception as e:
            attempts += 1
            if attempts > retry_count:
                return {
                    "feedback": item,
                    "error": str(e),
                    "status": "failed"
                }
            await asyncio.sleep(1)  # Backoff before retry
async def batch_process(items: List[str],
                        chunk_size: int = 10,
                        concurrency_limit: int = 5,
                        output_file: Optional[str] = None):

    sem = asyncio.Semaphore(concurrency_limit)
    results = []
    processed = 0
    total = len(items)
for i in range(0, total, chunk_size):
        chunk = items[i:i+chunk_size]

        # Create rate-limited processing function
        async def process_with_sem(item):
            async with sem:
                return await process_item(item)

        # Process current chunk of items
        tasks = [process_with_sem(item) for item in chunk]
        chunk_results = await asyncio.gather(*tasks)
        results.extend(chunk_results)

        # Track and display progress
        processed += len(chunk)
        print(f"Progress: {processed}/{total} ({processed/total*100:.1f}%)")

        # Save intermediate results to prevent data loss
        if output_file:
            with open(output_file, "a") as f:
                for result in chunk_results:
                    f.write(json.dumps(result) + "\n")

    return results
async def main():
    # Test with sample feedback data
    feedback_items = [
        "Your app crashes every time I try to upload a photo. Please fix this ASAP!",
        "I love the new dark mode feature. It makes the app much easier on the eyes.",
        "The checkout process is too complicated. I gave up trying to make a purchase.",
        "Your customer service rep was very helpful in resolving my issue."
        # Examples of different types of feedback
    ]

    results = await batch_process(
        items=feedback_items,
        output_file="feedback_results.jsonl"
    )
success_count = sum(1 for r in results if r["status"] == "success")
    print(f"Successfully processed: {success_count}/{len(results)}")

    # Analyze average priority of feedback items
    priorities = [r.get("priority", 0) for r in results if r["status"] == "success"]
    if priorities:
        print(f"Average priority: {sum(priorities)/len(priorities):.1f}")

    # Generate distribution of feedback categories
    categories = {}
    for r in results:
        if r["status"] == "success":
            for cat in r.get("categories", []):
                categories[cat] = categories.get(cat, 0) + 1

    print("Category distribution:")
    for cat, count in categories.items():
        print(f"  {cat}: {count}")

if __name__ == "__main__":
    asyncio.run(main())
```

```shell
$ pip install instructor pydantic
$ python batch-processing.py
```

For more information, see the original documentation:
- https://github.com/jxnl/instructor

### Hooks and Callbacks

```python
import instructor
import openai
import pprint
from pydantic import BaseModel
client = instructor.from_openai(openai.OpenAI())
class User(BaseModel):
    name: str
    age: int
def log_completion_kwargs(*args, **kwargs):
    """Log all arguments passed to the completion function."""
    print("Arguments sent to completion:")
    pprint.pprint(kwargs)

def log_completion_response(response):
    """Log the raw response from the API."""
    print("API Response received:")
    print(f"Model: {response.model}")
    print(f"Usage: {response.usage.total_tokens} tokens")

def handle_error(error):
    """Handle any errors that occur during completion."""
    print(f"Error type: {type(error).__name__}")
    print(f"Error message: {str(error)}")
client.on("completion:kwargs", log_completion_kwargs)
client.on("completion:response", log_completion_response)
client.on("completion:error", handle_error)
client.on("parse:error", handle_error)
user = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
        {"role": "user", "content": "Extract the user info: John is 25 years old."}
    ],
    response_model=User
)

print("Extracted user:", user)
client.off("completion:kwargs", log_completion_kwargs)
client.clear("completion:error")
client.clear()
import time
import instructor
import openai
from pydantic import BaseModel
client = instructor.from_openai(openai.OpenAI())
class Metrics:
    def __init__(self):
        self.request_times = []
        self.token_counts = []
        self.error_count = 0
        self.request_start_time = None

    def start_request(self, *args, **kwargs):
        self.request_start_time = time.time()

    def end_request(self, response):
        if self.request_start_time is not None:
            elapsed = time.time() - self.request_start_time
            self.request_times.append(elapsed)
            self.token_counts.append(response.usage.total_tokens)
            print(f"Request completed in {elapsed:.2f}s, {response.usage.total_tokens} tokens")

    def record_error(self, error):
        self.error_count += 1
        print(f"Error recorded: {str(error)}")

    def report(self):
        if not self.request_times:
            return "No requests recorded."

        avg_time = sum(self.request_times) / len(self.request_times)
        avg_tokens = sum(self.token_counts) / len(self.token_counts)
        total_tokens = sum(self.token_counts)

        return {
            "total_requests": len(self.request_times),
            "avg_request_time": f"{avg_time:.2f}s",
            "avg_tokens_per_request": int(avg_tokens),
            "total_tokens": total_tokens,
            "error_count": self.error_count
        }
metrics = Metrics()
client.on("completion:kwargs", metrics.start_request)
client.on("completion:response", metrics.end_request)
client.on("completion:error", metrics.record_error)
client.on("parse:error", metrics.record_error)
class Product(BaseModel):
    name: str
    price: float
    category: str

for i, query in enumerate([
    "iPhone 13, $799, Smartphones",
    "Air Fryer, $129.99, Kitchen Appliances",
    "Nike Running Shoes, $89.95, Athletic Footwear"
]):
    try:
        product = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "user", "content": f"Extract product info: {query}"}
            ],
            response_model=Product
        )
        print(f"Product {i+1}: {product.name}, ${product.price}, {product.category}")
    except Exception as e:
        print(f"Failed to extract product {i+1}: {e}")
performance_report = metrics.report()
print("\nPerformance Report:")
for key, value in performance_report.items():
    print(f"  {key}: {value}")
```

```shell
$ pip install instructor pydantic
$ python hooks-and-callbacks.py
```

For more information, see the original documentation:
- https://github.com/jxnl/instructor

### Type Adapters

```python
from typing import List, Dict, Any
from pydantic import TypeAdapter, BaseModel
import instructor
from openai import OpenAI
client = instructor.from_openai(OpenAI())
class User(BaseModel):
    name: str
    age: int
    skills: List[str]
UserListAdapter = TypeAdapter(List[User])
def extract_users_from_text(text: str) -> List[User]:
    # Get raw JSON data from LLM
    raw_data = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {
                "role": "user",
                "content": f"Extract all users from this text as a JSON array: {text}"
            }
        ],
        response_format={"type": "json_object"},
        temperature=0
    ).choices[0].message.content

    # Parse JSON
    import json
    try:
        data = json.loads(raw_data)
        # Use type adapter for validation
        users = UserListAdapter.validate_python(data.get("users", []))
        return users
    except Exception as e:
        print(f"Error parsing data: {e}")
        return []
text = """
Team members:
- John Smith, 32 years old, skills: Python, JavaScript, Docker
- Maria Garcia, 28 years old, skills: UX Design, Figma, HTML/CSS
- Alex Johnson, 35 years old, skills: Project Management, Agile, Scrum
"""

users = extract_users_from_text(text)
for user in users:
    print(f"{user.name} ({user.age}): {', '.join(user.skills)}")
from typing import Dict, List, Union, Any
from pydantic import TypeAdapter, BaseModel, Field
class Comment(BaseModel):
    user: str
    text: str
    timestamp: str

class Post(BaseModel):
    id: int
    title: str
    content: str
    tags: List[str]
    comments: List[Comment]
CommentAdapter = TypeAdapter(Comment)
PostAdapter = TypeAdapter(Post)
PostDictAdapter = TypeAdapter(Dict[str, Post])
def process_comment(raw_comment: Dict[str, Any]) -> Comment:
    return CommentAdapter.validate_python(raw_comment)

def process_post(raw_post: Dict[str, Any]) -> Post:
    return PostAdapter.validate_python(raw_post)

def process_posts_dict(raw_posts: Dict[str, Any]) -> Dict[str, Post]:
    return PostDictAdapter.validate_python(raw_posts)
raw_comment = {
    "user": "alice",
    "text": "Great post!",
    "timestamp": "2023-06-15T14:30:00Z"
}

raw_post = {
    "id": 1,
    "title": "Introduction to Type Adapters",
    "content": "Type adapters are a powerful feature...",
    "tags": ["pydantic", "python", "validation"],
    "comments": [
        raw_comment,
        {"user": "bob", "text": "Thanks for sharing!", "timestamp": "2023-06-15T15:45:00Z"}
    ]
}
comment = process_comment(raw_comment)
post = process_post(raw_post)

print(f"Comment by {comment.user}: {comment.text}")
print(f"Post: {post.title} with {len(post.comments)} comments and tags: {', '.join(post.tags)}")
from typing import List, Dict, Any, Optional
from pydantic import TypeAdapter, BaseModel, Field
class Address(BaseModel):
    street: str
    city: str
    postal_code: str
    country: str

class ContactInfo(BaseModel):
    email: str
    phone: Optional[str] = None
    address: Address

class Customer(BaseModel):
    id: str
    name: str
    contact_info: ContactInfo
    account_type: str
    active: bool
AddressAdapter = TypeAdapter(Address)
ContactInfoAdapter = TypeAdapter(ContactInfo)
CustomerAdapter = TypeAdapter(Customer)
CustomerListAdapter = TypeAdapter(List[Customer])
def process_customers(data: Dict[str, Any]) -> List[Customer]:
    try:
        # Extract customer data from a complex API response
        customers_data = data.get("results", {}).get("customers", [])
        return CustomerListAdapter.validate_python(customers_data)
    except Exception as e:
        print(f"Validation error: {e}")
        return []
api_response = {
    "status": "success",
    "results": {
        "customers": [
            {
                "id": "cust-001",
                "name": "Acme Corporation",
                "contact_info": {
                    "email": "contact@acme.com",
                    "phone": "555-123-4567",
                    "address": {
                        "street": "123 Main St",
                        "city": "San Francisco",
                        "postal_code": "94105",
                        "country": "USA"
                    }
                },
                "account_type": "enterprise",
                "active": True
            },
            {
                "id": "cust-002",
                "name": "Globex Inc",
                "contact_info": {
                    "email": "info@globex.com",
                    "address": {
                        "street": "456 Market St",
                        "city": "New York",
                        "postal_code": "10001",
                        "country": "USA"
                    }
                },
                "account_type": "small_business",
                "active": True
            }
        ]
    }
}
customers = process_customers(api_response)
print(f"Processed {len(customers)} valid customers")
for customer in customers:
    print(f"- {customer.name} ({customer.id})")
    print(f"  Email: {customer.contact_info.email}")
    print(f"  Address: {customer.contact_info.address.city}, {customer.contact_info.address.country}")
```

```shell
$ pip install instructor pydantic
$ python type-adapters.py
```

For more information, see the original documentation:
- https://github.com/jxnl/instructor

## Miscellaneous Examples

Additional examples that don't fit into other categories

### Working with Enums

Use enumerated types with Instructor for consistent, validated extractions. Enums help enforce a fixed set of allowed values.

```python
from enum import Enum
from pydantic import BaseModel
import instructor
from openai import OpenAI
class ProductCategory(str, Enum):
    ELECTRONICS = "electronics"
    CLOTHING = "clothing"
    HOME = "home"
    BOOKS = "books"
    TOYS = "toys"
class Product(BaseModel):
    name: str
    price: float
    category: ProductCategory
client = instructor.from_openai(OpenAI())
product = client.chat.completions.create(
    model="gpt-3.5-turbo",
    response_model=Product,
    messages=[
        {"role": "user", "content": "Our new wireless headphones cost $79.99 and belong in our electronics department."}
    ]
)
print(f"Product: {product.name}")
print(f"Price: ${product.price}")
print(f"Category: {product.category}")
```

```shell
$ pip install instructor pydantic
$ python working-with-enums.py
```

For more information, see the original documentation:
- https://github.com/jxnl/instructor

### Resources

```shell
$ pip install instructor pydantic
$ python resources.py
```

For more information, see the original documentation:
- https://github.com/instructor-ai/instructor
- https://python.useinstructor.com/
- https://discord.gg/bD9YE9JArw
- https://twitter.com/jxnlco
- https://python.useinstructor.com/blog
- https://github.com/instructor-ai/instructor/tree/main/examples
- https://python.useinstructor.com/
- https://github.com/instructor-ai/instructor/issues
- https://discord.gg/bD9YE9JArw
- https://github.com/instructor-ai/instructor/issues/new
- https://github.com/instructor-ai/instructor
- https://discord.gg/bD9YE9JArw
- https://python.useinstructor.com/blog
- https://github.com/instructor-ai/instructor/blob/main/CONTRIBUTING.md
- https://github.com/instructor-ai/instructor/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22
- https://docs.pydantic.dev/

### Untitled Example

